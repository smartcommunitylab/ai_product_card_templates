models:
   - name:                    "" # name of the model
     description:             "" # description of the model
     tags:                    "" # label the model to be easily findable and accessible
     version:                 "" # the version of the dataset
     framework:               "" # the primary framework the model is trained on (fasttext, pytorch, ....)
     artifacts:               "" # the path/url the model is being output (implementation/models)
       - reference_file:
         parameters:
           learning_rate:     ""
           epochs:            ""
         dataset:             "" # uri to the particular dataset used for training the model
         metrics:   
           
     training:
       output_dir:            "../implementation/models" # the path/url the model is being output (implementation/models)
       data:                   
         reference:           "../implementation/data" # (local/remote) (array of references?)
       implementation:         
         runtime:             "python" # the execution environment (python)
         source:              "../implementation/src/functions/train.py" # reference to the implementation of the training procedure inside implementation/src/
         requirements:        "implementation/src/requirements.txt" # particular dependencies the model training depends on
         resources:              # particular resources the model training depends on
           gpu:  1 A40           #
           vram: 10GB            #
       parameters: 
          language:           "it" #
          max_grad_norm:      5 # Gradient clipping norm.
          threshold:          0.5 # Threshold for the prediction confidence.
          fp16:               False # Whether to use 16-bit (mixed) precision training.
          learning_rate:      3e-5 #
          epochs:             1 #
          report_to:          "wandb" #
          batch_size:         8 #
          seeds:              "all" # Seeds to be used to load the data splits, separated by a comma (e.g. 110,221). Use 'all' to use all the data splits
          device:             "cpu" # Device to train on. choices=["cpu", "cuda"]
          custom_loss:        False # Enable the custom loss (focal loss by default)
          weighted_loss:      False # Enable the weighted bcewithlogits loss. Only works if the custom loss is enabled.
          eval_metric:        ""  # Evaluation metric to use on the validation set. 
                                  #choices=[
                                  #  'loss', 'f1_micro', 'f1_macro', 'f1_weighted', 'f1_samples',
                                  #  'jaccard_micro', 'jaccard_macro', 'jaccard_weighted', 'jaccard_samples',
                                  #  'matthews_macro', 'matthews_micro',
                                  #  'roc_auc_micro', 'roc_auc_macro', 'roc_auc_weighted', 'roc_auc_samples',
                                  #  'precision_micro', 'precision_macro', 'precision_weighted', 'precision_samples',
                                  #  'recall_micro', 'recall_macro', 'recall_weighted', 'recall_samples',
                                  #  'hamming_loss', 'accuracy', 'ndcg_1', 'ndcg_3', 'ndcg_5', 'ndcg_10']
     evaluation:
       - type:                "" # supported compliance library for the evaluations to be applied to the model  Evidently, Custom script ....
         definition:             # inline / url (local/remote)
         implementation:       
           runtime:           "" # the execution environment (python)
           source:            "" # reference to the implementation of the compliance procedure inside implementation/src/
           handler:           "" # the name of the function call
           requirements:         # particular dependencies the compliance execution depends on
           resources:            # particular resources the compliance depends on 
        metrics:
          f1_score:
            min_val:  
            max_val: 
     inference:
       parameters:
         top_k:               2 # Number of labels to return. If None, all the labels will be returned
         threshold:           0.5 # Threshold for the predictions
         device:              "cpu" # evice to use for the inference

     monitoring:
      paramteters: