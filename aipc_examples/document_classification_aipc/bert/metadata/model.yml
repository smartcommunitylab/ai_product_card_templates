models:
   - name:                    "bert-legal-acts-classification" # name of the model
     description:             "BERT model for multi-label text classification" # description of the model
     tags:                    "bert, text classification, legal acts classification" # label the model to be easily findable and accessible
     version:                 "0.1" # the version of the dataset
     framework:               "transformers" # the primary framework the model is trained on (fasttext, pytorch, ....)
     artifacts:               
       - reference_file:      "../implementation/models/text_classific_bert" # the path/url the model is being output (implementation/models)
         parameters:
           learning_rate:     ""
           epochs:            ""
         dataset:             "" # uri to the particular dataset used for training the model
         metrics:             ""
           
     training:
       output_dir:            "../../models" # the path/url the model is being output (implementation/models)
       bucket_name:           "bert-model"
       data:                   
         reference:           "../../data" # (local/remote)
       implementation:         
         runtime:             "python" # the execution environment (python)
         source:              "functions/train.py" # reference to the implementation of the training procedure inside implementation/src/
         handler:             "start_train" # the name of the function to call
         requirements:        "requirements.txt" # particular dependencies the model training depends on
         resources:              # particular resources the model training depends on
           gpu:  1 A40           #
           vram: 10GB            #
       parameters: 
          language:           "it" #
          max_grad_norm:      5 # Gradient clipping norm.
          threshold:          0.5 # Threshold for the prediction confidence.
          fp16:               False # Whether to use 16-bit (mixed) precision training.
          learning_rate:      "3e-5" #
          epochs:             1 #
          report_to:          "wandb" #
          batch_size:         8 #
          seeds:              "all" # Seeds to be used to load the data splits, separated by a comma (e.g. 110,221). Use 'all' to use all the data splits
          device:             "cpu" # Device to train on. choices=["cpu", "cuda"]
          custom_loss:        False # Enable the custom loss (focal loss by default)
          weighted_loss:      False # Enable the weighted bcewithlogits loss. Only works if the custom loss is enabled.
          full_metrics:       False # Compute all the metrics during the evaluation
          trust_remote:       False # Compute all the metrics during the evaluation
          save_class_report:  False # Save the classification report
          class_report_step:  1 # Number of epochs before creating a new classification report 
          eval_metric:        "f1_micro"  # Evaluation metric to use on the validation set. 
                                  #choices=[
                                  #  'loss', 'f1_micro', 'f1_macro', 'f1_weighted', 'f1_samples',
                                  #  'jaccard_micro', 'jaccard_macro', 'jaccard_weighted', 'jaccard_samples',
                                  #  'matthews_macro', 'matthews_micro',
                                  #  'roc_auc_micro', 'roc_auc_macro', 'roc_auc_weighted', 'roc_auc_samples',
                                  #  'precision_micro', 'precision_macro', 'precision_weighted', 'precision_samples',
                                  #  'recall_micro', 'recall_macro', 'recall_weighted', 'recall_samples',
                                  #  'hamming_loss', 'accuracy', 'ndcg_1', 'ndcg_3', 'ndcg_5', 'ndcg_10']
     evaluation:
       - type:                "custom" # supported compliance library for the evaluations to be applied to the model  Evidently, Custom script ....
         definition:          "" # inline / url (local/remote)
         implementation:       
           runtime:           "python" # the execution environment (python)
           source:            "functions/evaluate.py" # reference to the implementation of the compliance procedure inside implementation/src/
           handler:           "start_evaluate" # the name of the function call
           requirements:      "" # particular dependencies the compliance execution depends on
           resources:         "" # particular resources the compliance depends on 
         metrics:
          - name:             "f1_score"           
            min_val:          "" #
            max_val:          "" #
     inference:
       parameters:
         top_k:                2 # Number of labels to return. If None, all the labels will be returned
         threshold:            0.5 # Threshold for the predictions
         device:               "cpu" # evice to use for the inference
       serving:
         implementation:
           runtime:           "python" # the execution environment (python)
           source:            "serving/model_serving.py" # reference to the implementation of the inference procedure inside implementation/src/
           class_name:        "ClassifierModel" # the name of the serving class
           requirements:      "" # particular dependencies the inference execution depends on
           resources:         "" # particular resources the inference depends on

     monitoring:                  #
      paramteters:             "" #