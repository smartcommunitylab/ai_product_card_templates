{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5fda0c2a-2b3c-47c2-a5f1-b779389dc7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq pypdf faiss-gpu pandas SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d3f59f28-1e6c-4f80-b501-aab12c812d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from urllib.request import urlretrieve\n",
    "from sqlalchemy import create_engine\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59241b21-a406-4485-87aa-87f8c40458ba",
   "metadata": {},
   "source": [
    "# Persist prompt-response experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "98d49a53-f26f-47a8-ab48-3c96ee34738c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>llm</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>relevant_documents</th>\n",
       "      <th>datetime</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prompt, response, llm, embedding_model, relevant_documents, datetime, source]\n",
       "Index: []"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## run the first time we create the db of prompts\n",
    "engine = create_engine('sqlite:///prompts_history.db')\n",
    "table_name = 'prompts'\n",
    "columns = [\"prompt\", \"response\", \"llm\", \"embedding_model\", \"relevant_documents\", \"datetime\", \"source\"]\n",
    "if not os.path.isfile(\"prompts_history.db\"):\n",
    "    row = [{el:\"\" for el in columns}]\n",
    "    prompts_df = pd.DataFrame(columns=columns)\n",
    "else:\n",
    "    query = \"SELECT * FROM prompts\"\n",
    "    prompts_df = pd.read_sql(query, engine)\n",
    "prompts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54762c3-4124-48b5-abbe-af869709c7e5",
   "metadata": {},
   "source": [
    "# Preare documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f98e18-cd89-4b5f-b252-2a6c8ab2f861",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9bb1f0a6-3db3-430b-89aa-f37abfd553e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "loader = DirectoryLoader('model_cards/', glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b39643-da2d-42c5-9d65-fa227d4794e0",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8795d632-2b17-4abe-9762-c5cef1079780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=16000,\n",
    "                                               chunk_overlap=2000,\n",
    "                                               separators=['\\n', '.'])\n",
    "docs_before_split = loader.load()\n",
    "docs_after_split = text_splitter.split_documents(docs_before_split)\n",
    "len(docs_after_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8648dbc-de8a-4eb2-ad11-7b60a271b6fa",
   "metadata": {},
   "source": [
    "Documents should be:\n",
    "\n",
    "- large enough to contain enough information to answer a question, and\n",
    "- small enough to fit into the LLM prompt: Mistral-7B-v0.1 input tokens limited to 4096 tokens\n",
    "- small enough to fit into the embeddings model: BAAI/bge-small-en-v1.5: input tokens limited to 512 tokens (roughly 2000 characters. Note: 1 token ~ 4 characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "52f79293-bbfa-4482-86e1-e49e3a1c95d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before split, there were 283 documents loaded, with average characters equal to 2482.\n",
      "After split, there were 285 documents (chunks), with average characters equal to 2491 (average chunk length).\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs])//len(docs)\n",
    "avg_char_before_split = avg_doc_length(docs_before_split)\n",
    "avg_char_after_split = avg_doc_length(docs_after_split)\n",
    "\n",
    "print(f'Before split, there were {len(docs_before_split)} documents loaded, with average characters equal to {avg_char_before_split}.')\n",
    "print(f'After split, there were {len(docs_after_split)} documents (chunks), with average characters equal to {avg_char_after_split} (average chunk length).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e9a3e-11cb-4cc5-85a3-ed57796cdbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e8d0991-f4f1-4059-81ec-c2476972f095",
   "metadata": {},
   "source": [
    "# Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5ac13b7-42d5-4e84-8d46-81587ddd2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",  # alternatively use \"sentence-transformers/all-MiniLM-l6-v2\" for a light and faster experience.\n",
    "    model_kwargs={'device':'cpu'}, \n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e8f04cf4-d57b-496d-a1d3-685a78d13d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the embedding:  (384,)\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(huggingface_embeddings.embed_query(docs_after_split[0].page_content))\n",
    "#print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdeec4a-e181-4356-8ea7-762ebd46ffda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fd40c92-353c-428f-98d6-fffa7511a7a1",
   "metadata": {},
   "source": [
    "# RETRIEVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f47bc7-9ebd-4f35-8688-cb92891ec897",
   "metadata": {},
   "source": [
    "## Retrieval System for Vector Embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa0a9ff4-6788-48ed-ac88-71034b9fb966",
   "metadata": {},
   "source": [
    "Once we have a embedding model, we are ready to vectorize all our documents and store them in a vector store to construct a retrieval system. With specifically designed searching algorithms, a retrieval system can do similarity searching efficiently to retrieve relevant documents.\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. It solves limitations of traditional query search engines that are optimized for hash-based searches, and provides more scalable similarity search functions (nearest-neighbor search implementations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43383945-f429-4704-90d4-c9cbef0c6c54",
   "metadata": {},
   "source": [
    "### FAISS as a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1f544b91-828b-42c4-b3db-f73a453664ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name = 'prompts'\n",
    "# Export the DataFrame to the SQLite database\n",
    "geolocation_varchi.to_sql(table_name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e51627bb-1296-4c0e-bd06-18ecb051ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs_after_split, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e3143f0b-c88b-4287-a7f2-699ac796847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"What is the model that can be used for legal document classification?\"\"\"  \n",
    "relevant_documents = vectorstore.similarity_search(query)\n",
    "#relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "445bc460-4ea1-4e6b-bf1e-d5b881f6837b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'model_cards/Elron/bleurt-tiny-512/README.md'}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "fa4f9856-1802-4e32-98e5-c986a7d76e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>llm</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>relevant_documents</th>\n",
       "      <th>datetime</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\ntags:\\n- text-classificatio...</td>\n",
       "      <td>2024-06-18 14:41:02.575937</td>\n",
       "      <td>model_cards/Elron/bleurt-tiny-512/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\ninference: false\\nlicense: ...</td>\n",
       "      <td>2024-06-18 14:41:02.575937</td>\n",
       "      <td>model_cards/Fujitsu/AugCode/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\nlanguage: en\\nlicense: apac...</td>\n",
       "      <td>2024-06-18 14:41:02.575937</td>\n",
       "      <td>model_cards/Hate-speech-CNERG/bert-base-uncase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\nlanguage:\\n- da\\nlicense: a...</td>\n",
       "      <td>2024-06-18 14:41:02.575937</td>\n",
       "      <td>model_cards/alexandrainst/da-ned-base/README.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt response llm  \\\n",
       "0  What is the model that can be used for legal d...                \n",
       "1  What is the model that can be used for legal d...                \n",
       "2  What is the model that can be used for legal d...                \n",
       "3  What is the model that can be used for legal d...                \n",
       "\n",
       "  embedding_model                                 relevant_documents  \\\n",
       "0                  page_content='---\\ntags:\\n- text-classificatio...   \n",
       "1                  page_content='---\\ninference: false\\nlicense: ...   \n",
       "2                  page_content='---\\nlanguage: en\\nlicense: apac...   \n",
       "3                  page_content='---\\nlanguage:\\n- da\\nlicense: a...   \n",
       "\n",
       "                    datetime  \\\n",
       "0 2024-06-18 14:41:02.575937   \n",
       "1 2024-06-18 14:41:02.575937   \n",
       "2 2024-06-18 14:41:02.575937   \n",
       "3 2024-06-18 14:41:02.575937   \n",
       "\n",
       "                                              source  \n",
       "0        model_cards/Elron/bleurt-tiny-512/README.md  \n",
       "1              model_cards/Fujitsu/AugCode/README.md  \n",
       "2  model_cards/Hate-speech-CNERG/bert-base-uncase...  \n",
       "3    model_cards/alexandrainst/da-ned-base/README.md  "
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = datetime.now()\n",
    "for doc in relevant_documents:\n",
    "    prompts_df.loc[len(prompts_df)] = {\n",
    "        \"prompt\": query, \n",
    "        \"response\": \"\", \n",
    "        \"llm\": \"\", \n",
    "        \"embedding_model\": \"\",\n",
    "        \"relevant_documents\": str(doc), \n",
    "        \"datetime\": date,\n",
    "        \"source\": str(doc.metadata[\"source\"])}\n",
    "prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "73cdfdef-a9bb-4321-bd22-db7c6d3f7484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_df.to_sql(table_name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "9ea6a740-1974-4842-bb78-686c3def3817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 documents retrieved which are relevant to the query. Display the first one:\n",
      "\n",
      "---\n",
      "tags:\n",
      "- text-classification\n",
      "- bert\n",
      "---\n",
      "\n",
      "# Model Card for bleurt-tiny-512 \n",
      " \n",
      "# Model Details\n",
      " \n",
      "## Model Description\n",
      " \n",
      "Pytorch version of the original BLEURT models from ACL paper\n",
      " \n",
      "- **Developed by:** Elron Bandel, Thibault Sellam, Dipanjan Das and Ankur P. Parikh of Google Research\n",
      "- **Shared by [Optional]:** Elron Bandel\n",
      "- **Model type:** Text Classification \n",
      "- **Language(s) (NLP):** More information needed\n",
      "- **License:** More information needed \n",
      "- **Parent Model:** BERT\n",
      "- **Resources for more information:**\n",
      "     - [GitHub Repo](https://github.com/google-research/bleurt/tree/master)\n",
      " \t  - [Associated Paper](https://aclanthology.org/2020.acl-main.704/)\n",
      "    - [Blog Post](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)\n",
      " \t\n",
      "\n",
      "\n",
      "# Uses\n",
      " \n",
      "\n",
      "## Direct Use\n",
      "This model can be used for the task of Text Classification \n",
      " \n",
      "## Downstream Use [Optional]\n",
      " \n",
      "More information needed.\n",
      " \n",
      "## Out-of-Scope Use\n",
      " \n",
      "The model should not be used to intentionally create hostile or alienating environments for people. \n",
      " \n",
      "# Bias, Risks, and Limitations\n",
      " \n",
      " \n",
      "Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n",
      "\n",
      "\n",
      "\n",
      "## Recommendations\n",
      " \n",
      " \n",
      "Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n",
      "\n",
      "# Training Details\n",
      " \n",
      "## Training Data\n",
      "The model authors note in the [associated paper](https://aclanthology.org/2020.acl-main.704.pdf): \n",
      "> We use years 2017 to 2019 of the WMT Metrics Shared Task, to-English language pairs. For each year, we used the of- ficial WMT test set, which include several thou- sand pairs of sentences with human ratings from the news domain. The training sets contain 5,360, 9,492, and 147,691 records for each year. \n",
      " \n",
      " \n",
      "## Training Procedure\n",
      "\n",
      " \n",
      "### Preprocessing\n",
      " \n",
      "More information needed \n",
      " \n",
      "### Speeds, Sizes, Times\n",
      "More information needed \n",
      "\n",
      " \n",
      "# Evaluation\n",
      " \n",
      " \n",
      "## Testing Data, Factors & Metrics\n",
      " \n",
      "### Testing Data\n",
      " \n",
      "The test sets for years 2018 and 2019 [of the WMT Metrics Shared Task, to-English language pairs.]  are noisier,\n",
      " \n",
      " \n",
      " \n",
      "### Factors\n",
      "More information needed\n",
      " \n",
      "### Metrics\n",
      " \n",
      "More information needed\n",
      " \n",
      " \n",
      "## Results \n",
      " \n",
      "More information needed\n",
      "\n",
      " \n",
      "# Model Examination\n",
      " \n",
      "More information needed\n",
      " \n",
      "# Environmental Impact\n",
      " \n",
      "Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n",
      " \n",
      "- **Hardware Type:** More information needed\n",
      "- **Hours used:** More information needed\n",
      "- **Cloud Provider:** More information needed\n",
      "- **Compute Region:** More information needed\n",
      "- **Carbon Emitted:** More information needed\n",
      " \n",
      "# Technical Specifications [optional]\n",
      " \n",
      "## Model Architecture and Objective\n",
      "\n",
      "More information needed \n",
      " \n",
      "## Compute Infrastructure\n",
      " \n",
      "More information needed \n",
      " \n",
      "### Hardware\n",
      " \n",
      " \n",
      "More information needed\n",
      " \n",
      "### Software\n",
      " \n",
      "More information needed.\n",
      " \n",
      "# Citation\n",
      "\n",
      " \n",
      "**BibTeX:**\n",
      " \n",
      " \n",
      "```bibtex\n",
      "@inproceedings{sellam2020bleurt,\n",
      "  title = {BLEURT: Learning Robust Metrics for Text Generation},\n",
      "  author = {Thibault Sellam and Dipanjan Das and Ankur P Parikh},\n",
      "  year = {2020},\n",
      "  booktitle = {Proceedings of ACL}\n",
      "}\n",
      "```\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "# Glossary [optional]\n",
      "More information needed \n",
      " \n",
      "# More Information [optional]\n",
      "More information needed \n",
      "\n",
      " \n",
      "# Model Card Authors [optional]\n",
      " \n",
      " Elron Bandel in collaboration with Ezi Ozoani and the Hugging Face team\n",
      "\n",
      "\n",
      "# Model Card Contact\n",
      " \n",
      "More information needed\n",
      " \n",
      "# How to Get Started with the Model\n",
      " \n",
      "Use the code below to get started with the model.\n",
      " \n",
      "<details>\n",
      "<summary> Click to expand </summary>\n",
      "\n",
      "```python\n",
      "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
      "import torch\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-tiny-512\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-tiny-512\")\n",
      "model.eval()\n",
      "\n",
      "references = [\"hello world\", \"hello world\"]\n",
      "candidates = [\"hi universe\", \"bye world\"]\n",
      "\n",
      "with torch.no_grad():\n",
      "  scores = model(**tokenizer(references, candidates, return_tensors='pt'))[0].squeeze()\n",
      "\n",
      "print(scores) # tensor([-0.9414, -0.5678])\n",
      " ```\n",
      "\n",
      "See [this notebook](https://colab.research.google.com/drive/1KsCUkFW45d5_ROSv2aHtXgeBa2Z98r03?usp=sharing) for model conversion code. \n",
      "</details>\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(relevant_documents)} documents retrieved which are relevant to the query. Display the first one:\\n')\n",
    "print(relevant_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c45b4-25f0-43d4-944d-f6e33681f184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d6fcf38-6348-4f9c-8451-e8decbe5365a",
   "metadata": {},
   "source": [
    "## Create a retriever interface using vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "2452b31d-c555-4e51-8247-107608e7f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use similarity searching algorithm and return 3 most relevant documents.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c31c85b8-1b85-4f58-b68e-4bb7074915a5",
   "metadata": {},
   "source": [
    "Now we have our vector store and retrieval system ready. \n",
    "We then need a large language model (LLM) to process information and answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792aab79-979b-4f06-94a3-816c6b60c7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc7f35d7-5ee6-4684-8c5b-b4e3cc715bef",
   "metadata": {},
   "source": [
    "# Open source LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9afce7de-e917-4f8f-87e0-22858b9d7f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the model that can be used for legal document classification? \\n-----------------------------------------------\\n\\nThere are several machine learning models that can be used for legal document classification, including:\\n\\n1. **Naive Bayes**: A simple probabilistic classifier that can be used for text classification tasks, including legal document classification.\\n2. **Support Vector Machines (SVMs)**: A popular machine learning algorithm that can be used for text classification tasks, including legal document classification.\\n3. **Random Forest**: An ensemble learning method that combines multiple decision trees to classify legal documents.\\n4'"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "hf = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=\"\",\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    model_kwargs={\"temperature\":0.1, \"max_length\":500})\n",
    "\n",
    "query = \"\"\"What is the model that can be used for legal document classification?\"\"\" \n",
    "hf.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "77a0e6e0-87f9-420a-91b6-515a4bef6b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:21<00:00,  5.40s/it]\n",
      "Device has 1 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"temperature\": 0.01, \"max_new_tokens\": 300}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ba750ff9-8b0b-4628-af0f-418af9da4abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**\\nA. Naive Bayes\\nB. Decision Trees\\nC. Support Vector Machines\\nD. Random Forest\\nAnswer: C. Support Vector Machines\\nExplanation: Support Vector Machines (SVM) is a popular machine learning model that can be used for legal document classification. SVM is a supervised learning model that can be used for classification and regression tasks. It is particularly effective for high-dimensional data and can handle non-linear relationships between the features and the target variable. SVM has been widely used in various applications, including text classification, image classification, and bioinformatics.\\n\\nIn the context of legal document classification, SVM can be used to classify documents into different categories such as contracts, agreements, lawsuits, and so on. The model can be trained on a dataset of labeled documents, and then used to classify new, unseen documents. SVM is a robust and accurate model that can handle the complexity of legal documents, which often contain complex language and nuanced concepts.\\n\\nThe other options are not as suitable for legal document classification:\\n\\nA. Naive Bayes is a simple probabilistic model that is not as effective for complex tasks like legal document classification.\\n\\nB. Decision Trees are a type of supervised learning model that can be used for classification tasks, but they are not as effective as SVM for high-dimensional data and complex relationships.\\n\\nD. Random Forest is an ensemble learning model that combines the predictions of multiple decision trees. While it can be effective for certain tasks, it is not as suitable for legal document classification as'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = hf \n",
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268127f-9edc-40ca-a904-662a31ba69eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2285c620-5b0b-4ffb-8ba0-868af471f2be",
   "metadata": {},
   "source": [
    "# Use together the retrieval system for relevant documents and the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "4df64ffa-c435-43fe-b289-441f16fafab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
    "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    " template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8547c7f8-a81e-414e-ab2d-5f33f663668c",
   "metadata": {},
   "source": [
    "Call LangChain’s RetrievalQA with the prompt above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8c8e5725-499e-4113-876c-f5350f20ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5c831-8f8c-40d2-9173-b55efb062ecc",
   "metadata": {},
   "source": [
    "## Use RetrievalQA invoke method to execute the chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62999093-6061-4a5f-9e36-c981779b1c35",
   "metadata": {},
   "source": [
    "### Option 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3cd37a4d-15be-428e-9dd9-2c2c9a0283a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a36fcfe-a713-41df-b847-449f6ae9938b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I can't find the final answer but you may want to check the following links: https://www.census.gov/topics/employment/industry-occupation/guidance/code-lists.html. The text does not mention Task Decomposition. It appears to be discussing job quality, work schedules, and occupations. If you're looking for information on Task Decomposition, you may want to search for a different source.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7d74a-c9be-4e31-9d1a-e2609588aca7",
   "metadata": {},
   "source": [
    "### Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "9ee812af-1f9c-4454-86c1-114f3feeae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model that can be used for legal document classification is the \"bleurt-tiny-512\" model. This model is a fine-tuned version of the BERT model and is specifically designed for text classification tasks, including legal document classification. It has been trained on a large dataset of legal documents and can be used to classify new documents into different categories based on their content.\n"
     ]
    }
   ],
   "source": [
    "# Call the QA chain with our query.\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "0555f584-e064-4492-b898-d2625425e8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>llm</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>relevant_documents</th>\n",
       "      <th>datetime</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\ntags:\\n- text-classificatio...</td>\n",
       "      <td>2024-06-18 14:41:02.575937</td>\n",
       "      <td>model_cards/Elron/bleurt-tiny-512/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\ninference: false\\nlicense: ...</td>\n",
       "      <td>2024-06-18 14:41:02.575937</td>\n",
       "      <td>model_cards/Fujitsu/AugCode/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\nlanguage: en\\nlicense: apac...</td>\n",
       "      <td>2024-06-18 14:41:02.575937</td>\n",
       "      <td>model_cards/Hate-speech-CNERG/bert-base-uncase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\nlanguage:\\n- da\\nlicense: a...</td>\n",
       "      <td>2024-06-18 14:41:02.575937</td>\n",
       "      <td>model_cards/alexandrainst/da-ned-base/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td>The model that can be used for legal document ...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\ntags:\\n- text-classificatio...</td>\n",
       "      <td>2024-06-18 14:56:54.166165</td>\n",
       "      <td>model_cards/Elron/bleurt-tiny-512/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td>The model that can be used for legal document ...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\ninference: false\\nlicense: ...</td>\n",
       "      <td>2024-06-18 14:56:54.166165</td>\n",
       "      <td>model_cards/Fujitsu/AugCode/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td>The model that can be used for legal document ...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\nlanguage: en\\nlicense: apac...</td>\n",
       "      <td>2024-06-18 14:56:54.166165</td>\n",
       "      <td>model_cards/Hate-speech-CNERG/bert-base-uncase...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  What is the model that can be used for legal d...   \n",
       "1  What is the model that can be used for legal d...   \n",
       "2  What is the model that can be used for legal d...   \n",
       "3  What is the model that can be used for legal d...   \n",
       "4  What is the model that can be used for legal d...   \n",
       "5  What is the model that can be used for legal d...   \n",
       "6  What is the model that can be used for legal d...   \n",
       "\n",
       "                                            response  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  The model that can be used for legal document ...   \n",
       "5  The model that can be used for legal document ...   \n",
       "6  The model that can be used for legal document ...   \n",
       "\n",
       "                                   llm         embedding_model  \\\n",
       "0                                                                \n",
       "1                                                                \n",
       "2                                                                \n",
       "3                                                                \n",
       "4  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "5  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "6  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "\n",
       "                                  relevant_documents  \\\n",
       "0  page_content='---\\ntags:\\n- text-classificatio...   \n",
       "1  page_content='---\\ninference: false\\nlicense: ...   \n",
       "2  page_content='---\\nlanguage: en\\nlicense: apac...   \n",
       "3  page_content='---\\nlanguage:\\n- da\\nlicense: a...   \n",
       "4  page_content='---\\ntags:\\n- text-classificatio...   \n",
       "5  page_content='---\\ninference: false\\nlicense: ...   \n",
       "6  page_content='---\\nlanguage: en\\nlicense: apac...   \n",
       "\n",
       "                    datetime  \\\n",
       "0 2024-06-18 14:41:02.575937   \n",
       "1 2024-06-18 14:41:02.575937   \n",
       "2 2024-06-18 14:41:02.575937   \n",
       "3 2024-06-18 14:41:02.575937   \n",
       "4 2024-06-18 14:56:54.166165   \n",
       "5 2024-06-18 14:56:54.166165   \n",
       "6 2024-06-18 14:56:54.166165   \n",
       "\n",
       "                                              source  \n",
       "0        model_cards/Elron/bleurt-tiny-512/README.md  \n",
       "1              model_cards/Fujitsu/AugCode/README.md  \n",
       "2  model_cards/Hate-speech-CNERG/bert-base-uncase...  \n",
       "3    model_cards/alexandrainst/da-ned-base/README.md  \n",
       "4        model_cards/Elron/bleurt-tiny-512/README.md  \n",
       "5              model_cards/Fujitsu/AugCode/README.md  \n",
       "6  model_cards/Hate-speech-CNERG/bert-base-uncase...  "
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = datetime.now()\n",
    "for doc in result['source_documents']:\n",
    "    prompts_df.loc[len(prompts_df)] = {\n",
    "        \"prompt\": query, \n",
    "        \"response\": result['result'], \n",
    "        \"llm\": \"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "        \"embedding_model\": \"BAAI/bge-small-en-v1.5\",\n",
    "        \"relevant_documents\": str(doc), \n",
    "        \"datetime\": date,\n",
    "        \"source\": str(doc.metadata[\"source\"])}\n",
    "prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "ee7b312e-329a-4d19-bfc7-e058a122d6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_df.to_sql(table_name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b0056d3c-513f-4562-97f9-5b087c0a82b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='---\\ntags:\\n- text-classification\\n- bert\\n---\\n\\n# Model Card for bleurt-tiny-512 \\n \\n# Model Details\\n \\n## Model Description\\n \\nPytorch version of the original BLEURT models from ACL paper\\n \\n- **Developed by:** Elron Bandel, Thibault Sellam, Dipanjan Das and Ankur P. Parikh of Google Research\\n- **Shared by [Optional]:** Elron Bandel\\n- **Model type:** Text Classification \\n- **Language(s) (NLP):** More information needed\\n- **License:** More information needed \\n- **Parent Model:** BERT\\n- **Resources for more information:**\\n     - [GitHub Repo](https://github.com/google-research/bleurt/tree/master)\\n \\t  - [Associated Paper](https://aclanthology.org/2020.acl-main.704/)\\n    - [Blog Post](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)\\n \\t\\n\\n\\n# Uses\\n \\n\\n## Direct Use\\nThis model can be used for the task of Text Classification \\n \\n## Downstream Use [Optional]\\n \\nMore information needed.\\n \\n## Out-of-Scope Use\\n \\nThe model should not be used to intentionally create hostile or alienating environments for people. \\n \\n# Bias, Risks, and Limitations\\n \\n \\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\\n\\n\\n\\n## Recommendations\\n \\n \\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\\n\\n# Training Details\\n \\n## Training Data\\nThe model authors note in the [associated paper](https://aclanthology.org/2020.acl-main.704.pdf): \\n> We use years 2017 to 2019 of the WMT Metrics Shared Task, to-English language pairs. For each year, we used the of- ficial WMT test set, which include several thou- sand pairs of sentences with human ratings from the news domain. The training sets contain 5,360, 9,492, and 147,691 records for each year. \\n \\n \\n## Training Procedure\\n\\n \\n### Preprocessing\\n \\nMore information needed \\n \\n### Speeds, Sizes, Times\\nMore information needed \\n\\n \\n# Evaluation\\n \\n \\n## Testing Data, Factors & Metrics\\n \\n### Testing Data\\n \\nThe test sets for years 2018 and 2019 [of the WMT Metrics Shared Task, to-English language pairs.]  are noisier,\\n \\n \\n \\n### Factors\\nMore information needed\\n \\n### Metrics\\n \\nMore information needed\\n \\n \\n## Results \\n \\nMore information needed\\n\\n \\n# Model Examination\\n \\nMore information needed\\n \\n# Environmental Impact\\n \\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\\n \\n- **Hardware Type:** More information needed\\n- **Hours used:** More information needed\\n- **Cloud Provider:** More information needed\\n- **Compute Region:** More information needed\\n- **Carbon Emitted:** More information needed\\n \\n# Technical Specifications [optional]\\n \\n## Model Architecture and Objective\\n\\nMore information needed \\n \\n## Compute Infrastructure\\n \\nMore information needed \\n \\n### Hardware\\n \\n \\nMore information needed\\n \\n### Software\\n \\nMore information needed.\\n \\n# Citation\\n\\n \\n**BibTeX:**\\n \\n \\n```bibtex\\n@inproceedings{sellam2020bleurt,\\n  title = {BLEURT: Learning Robust Metrics for Text Generation},\\n  author = {Thibault Sellam and Dipanjan Das and Ankur P Parikh},\\n  year = {2020},\\n  booktitle = {Proceedings of ACL}\\n}\\n```\\n \\n \\n \\n \\n# Glossary [optional]\\nMore information needed \\n \\n# More Information [optional]\\nMore information needed \\n\\n \\n# Model Card Authors [optional]\\n \\n Elron Bandel in collaboration with Ezi Ozoani and the Hugging Face team\\n\\n\\n# Model Card Contact\\n \\nMore information needed\\n \\n# How to Get Started with the Model\\n \\nUse the code below to get started with the model.\\n \\n<details>\\n<summary> Click to expand </summary>\\n\\n```python\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\nimport torch\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-tiny-512\")\\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-tiny-512\")\\nmodel.eval()\\n\\nreferences = [\"hello world\", \"hello world\"]\\ncandidates = [\"hi universe\", \"bye world\"]\\n\\nwith torch.no_grad():\\n  scores = model(**tokenizer(references, candidates, return_tensors=\\'pt\\'))[0].squeeze()\\n\\nprint(scores) # tensor([-0.9414, -0.5678])\\n ```\\n\\nSee [this notebook](https://colab.research.google.com/drive/1KsCUkFW45d5_ROSv2aHtXgeBa2Z98r03?usp=sharing) for model conversion code. \\n</details>', metadata={'source': 'model_cards/Elron/bleurt-tiny-512/README.md'}), Document(page_content='---\\ninference: false\\nlicense: mit\\nwidget:\\nlanguage:\\n- en\\nmetrics:\\n- mrr\\ndatasets:\\n- augmented_codesearchnet\\n---\\n#  🔥 Augmented Code Model 🔥\\nThis is Augmented Code Model which is a fined-tune model of [CodeBERT](https://huggingface.co/microsoft/codebert-base) for processing of similarity between given docstring and code. This model is fined-model based on Augmented Code Corpus with ACS=4. \\n\\n## How to use the model ?\\nSimilar to other huggingface model, you may load the model as follows.\\n```python\\n\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"Fujitsu/AugCode\")\\n\\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Fujitsu/AugCode\")\\n\\n```\\nThen you may use `model` to infer the similarity between a given docstring and code.\\n\\n### Citation\\n```bibtex@misc{bahrami2021augcode,\\n    title={AugmentedCode: Examining the Effects of Natural Language Resources in Code Retrieval Models},\\n    author={Mehdi Bahrami, N. C. Shrikanth, Yuji Mizobuchi, Lei Liu, Masahiro Fukuyori, Wei-Peng Chen, Kazuki Munakata},\\n    year={2021},\\n    eprint={TBA},\\n    archivePrefix={TBA},\\n    primaryClass={cs.CL}\\n}\\n```', metadata={'source': 'model_cards/Fujitsu/AugCode/README.md'}), Document(page_content='---\\nlanguage: en\\nlicense: apache-2.0\\ndatasets:\\n  - hatexplain\\n---\\n\\n\\n## Table of Contents\\n- [Model Details](#model-details)\\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\\n- [Uses](#uses)\\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\\n- [Training](#training)\\n- [Evaluation](#evaluation)\\n- [Technical Specifications](#technical-specifications)\\n- [Citation Information](#citation-information)\\n\\n## Model Details\\n**Model Description:** \\nThe model is used for classifying a text as Abusive (Hatespeech and Offensive) or Normal. The model is trained using data from Gab and Twitter and Human Rationales were included as part of the training data to boost the performance. The model also has a rationale predictor head that can predict the rationales given an abusive sentence\\n\\n\\n- **Developed by:** Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee \\n- **Model Type:** Text Classification\\n- **Language(s):**  English\\n- **License:**  Apache-2.0\\n- **Parent Model:** See the [BERT base uncased model](https://huggingface.co/bert-base-uncased) for more information about the BERT base model.\\n- **Resources for more information:**\\n  - [Research Paper](https://arxiv.org/abs/2012.10289) Accepted at AAAI 2021.\\n  - [GitHub Repo with datatsets and models](https://github.com/punyajoy/HateXplain)\\n\\n\\n                                                                                               \\n## How to Get Started with the Model\\n\\n**Details of usage**\\n\\nPlease use the **Model_Rational_Label** class inside [models.py](models.py) to load the models. The default prediction in this hosted inference API may be wrong due to the use of different class initialisations.\\n\\n```python\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\n### from models.py\\nfrom models import *\\ntokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\\nmodel = Model_Rational_Label.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\\ninputs = tokenizer(\\'He is a great guy\", return_tensors=\"pt\")\\nprediction_logits, _ = model(input_ids=inputs[\\'input_ids\\'],attention_mask=inputs[\\'attention_mask\\'])\\n```\\n\\n## Uses\\n\\n#### Direct Use\\n\\nThis model can be used for Text Classification\\n\\n\\n#### Downstream Use\\n\\n[More information needed]\\n\\n#### Misuse and Out-of-scope Use\\n\\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\\n\\n## Risks, Limitations and Biases\\n\\n**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\\n\\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).\\n\\n(and if you can generate an example of a biased prediction, also something like this): \\n\\nPredictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For ![example:](https://github.com/hate-alert/HateXplain/blob/master/Figures/dataset_example.png) \\n\\nThe model author\\'s also note in their HateXplain paper that they \\n>  *have not considered any external context such as profile bio, user gender, history of posts etc., which might be helpful in the classification task. Also, in this work we have focused on the English language. It does not consider multilingual hate speech into account.*\\n\\n\\n#### Training Procedure\\n\\n##### Preprocessing\\n\\nThe authors detail their preprocessing procedure in the [Github repository](https://github.com/hate-alert/HateXplain/tree/master/Preprocess)\\n\\n\\n## Evaluation\\nThe mode authors detail the Hidden layer size and attention for the HateXplain fien tuned models in the [associated paper](https://arxiv.org/pdf/2012.10289.pdf)\\n\\n#### Results \\n\\nThe model authors both in their paper and in the git repository provide the illustrative output of the BERT - HateXplain in comparison to BERT and and other HateXplain fine tuned ![models]( https://github.com/hate-alert/HateXplain/blob/master/Figures/bias-subgroup.pdf)\\n\\n## Citation Information\\n\\n```bibtex\\n@article{mathew2020hatexplain,\\n  title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},\\n  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},\\n  journal={arXiv preprint arXiv:2012.10289},\\n  year={2020}\\n\\n}\\n```', metadata={'source': 'model_cards/Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two/README.md'})]\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = result['source_documents']\n",
    "print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3e86c4bf-ae69-4d10-8f49-af0e47068e1b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 documents retrieved which are relevant to the query.\n",
      "****************************************************************************************************\n",
      "Relevant Document #1:\n",
      "Source file: model_cards/Elron/bleurt-tiny-512/README.md, \n",
      "Content: ---\n",
      "tags:\n",
      "- text-classification\n",
      "- bert\n",
      "---\n",
      "\n",
      "# Model Card for bleurt-tiny-512 \n",
      " \n",
      "# Model Details\n",
      " \n",
      "## Model Description\n",
      " \n",
      "Pytorch version of the original BLEURT models from ACL paper\n",
      " \n",
      "- **Developed by:** Elron Bandel, Thibault Sellam, Dipanjan Das and Ankur P. Parikh of Google Research\n",
      "- **Shared by [Optional]:** Elron Bandel\n",
      "- **Model type:** Text Classification \n",
      "- **Language(s) (NLP):** More information needed\n",
      "- **License:** More information needed \n",
      "- **Parent Model:** BERT\n",
      "- **Resources for more information:**\n",
      "     - [GitHub Repo](https://github.com/google-research/bleurt/tree/master)\n",
      " \t  - [Associated Paper](https://aclanthology.org/2020.acl-main.704/)\n",
      "    - [Blog Post](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)\n",
      " \t\n",
      "\n",
      "\n",
      "# Uses\n",
      " \n",
      "\n",
      "## Direct Use\n",
      "This model can be used for the task of Text Classification \n",
      " \n",
      "## Downstream Use [Optional]\n",
      " \n",
      "More information needed.\n",
      " \n",
      "## Out-of-Scope Use\n",
      " \n",
      "The model should not be used to intentionally create hostile or alienating environments for people. \n",
      " \n",
      "# Bias, Risks, and Limitations\n",
      " \n",
      " \n",
      "Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n",
      "\n",
      "\n",
      "\n",
      "## Recommendations\n",
      " \n",
      " \n",
      "Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n",
      "\n",
      "# Training Details\n",
      " \n",
      "## Training Data\n",
      "The model authors note in the [associated paper](https://aclanthology.org/2020.acl-main.704.pdf): \n",
      "> We use years 2017 to 2019 of the WMT Metrics Shared Task, to-English language pairs. For each year, we used the of- ficial WMT test set, which include several thou- sand pairs of sentences with human ratings from the news domain. The training sets contain 5,360, 9,492, and 147,691 records for each year. \n",
      " \n",
      " \n",
      "## Training Procedure\n",
      "\n",
      " \n",
      "### Preprocessing\n",
      " \n",
      "More information needed \n",
      " \n",
      "### Speeds, Sizes, Times\n",
      "More information needed \n",
      "\n",
      " \n",
      "# Evaluation\n",
      " \n",
      " \n",
      "## Testing Data, Factors & Metrics\n",
      " \n",
      "### Testing Data\n",
      " \n",
      "The test sets for years 2018 and 2019 [of the WMT Metrics Shared Task, to-English language pairs.]  are noisier,\n",
      " \n",
      " \n",
      " \n",
      "### Factors\n",
      "More information needed\n",
      " \n",
      "### Metrics\n",
      " \n",
      "More information needed\n",
      " \n",
      " \n",
      "## Results \n",
      " \n",
      "More information needed\n",
      "\n",
      " \n",
      "# Model Examination\n",
      " \n",
      "More information needed\n",
      " \n",
      "# Environmental Impact\n",
      " \n",
      "Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n",
      " \n",
      "- **Hardware Type:** More information needed\n",
      "- **Hours used:** More information needed\n",
      "- **Cloud Provider:** More information needed\n",
      "- **Compute Region:** More information needed\n",
      "- **Carbon Emitted:** More information needed\n",
      " \n",
      "# Technical Specifications [optional]\n",
      " \n",
      "## Model Architecture and Objective\n",
      "\n",
      "More information needed \n",
      " \n",
      "## Compute Infrastructure\n",
      " \n",
      "More information needed \n",
      " \n",
      "### Hardware\n",
      " \n",
      " \n",
      "More information needed\n",
      " \n",
      "### Software\n",
      " \n",
      "More information needed.\n",
      " \n",
      "# Citation\n",
      "\n",
      " \n",
      "**BibTeX:**\n",
      " \n",
      " \n",
      "```bibtex\n",
      "@inproceedings{sellam2020bleurt,\n",
      "  title = {BLEURT: Learning Robust Metrics for Text Generation},\n",
      "  author = {Thibault Sellam and Dipanjan Das and Ankur P Parikh},\n",
      "  year = {2020},\n",
      "  booktitle = {Proceedings of ACL}\n",
      "}\n",
      "```\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "# Glossary [optional]\n",
      "More information needed \n",
      " \n",
      "# More Information [optional]\n",
      "More information needed \n",
      "\n",
      " \n",
      "# Model Card Authors [optional]\n",
      " \n",
      " Elron Bandel in collaboration with Ezi Ozoani and the Hugging Face team\n",
      "\n",
      "\n",
      "# Model Card Contact\n",
      " \n",
      "More information needed\n",
      " \n",
      "# How to Get Started with the Model\n",
      " \n",
      "Use the code below to get started with the model.\n",
      " \n",
      "<details>\n",
      "<summary> Click to expand </summary>\n",
      "\n",
      "```python\n",
      "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
      "import torch\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-tiny-512\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-tiny-512\")\n",
      "model.eval()\n",
      "\n",
      "references = [\"hello world\", \"hello world\"]\n",
      "candidates = [\"hi universe\", \"bye world\"]\n",
      "\n",
      "with torch.no_grad():\n",
      "  scores = model(**tokenizer(references, candidates, return_tensors='pt'))[0].squeeze()\n",
      "\n",
      "print(scores) # tensor([-0.9414, -0.5678])\n",
      " ```\n",
      "\n",
      "See [this notebook](https://colab.research.google.com/drive/1KsCUkFW45d5_ROSv2aHtXgeBa2Z98r03?usp=sharing) for model conversion code. \n",
      "</details>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 3 documents retrieved which are relevant to the query.\n",
      "Relevant Document #2:\n",
      "Source file: model_cards/Fujitsu/AugCode/README.md, \n",
      "Content: ---\n",
      "inference: false\n",
      "license: mit\n",
      "widget:\n",
      "language:\n",
      "- en\n",
      "metrics:\n",
      "- mrr\n",
      "datasets:\n",
      "- augmented_codesearchnet\n",
      "---\n",
      "#  🔥 Augmented Code Model 🔥\n",
      "This is Augmented Code Model which is a fined-tune model of [CodeBERT](https://huggingface.co/microsoft/codebert-base) for processing of similarity between given docstring and code. This model is fined-model based on Augmented Code Corpus with ACS=4. \n",
      "\n",
      "## How to use the model ?\n",
      "Similar to other huggingface model, you may load the model as follows.\n",
      "```python\n",
      "\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"Fujitsu/AugCode\")\n",
      "\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"Fujitsu/AugCode\")\n",
      "\n",
      "```\n",
      "Then you may use `model` to infer the similarity between a given docstring and code.\n",
      "\n",
      "### Citation\n",
      "```bibtex@misc{bahrami2021augcode,\n",
      "    title={AugmentedCode: Examining the Effects of Natural Language Resources in Code Retrieval Models},\n",
      "    author={Mehdi Bahrami, N. C. Shrikanth, Yuji Mizobuchi, Lei Liu, Masahiro Fukuyori, Wei-Peng Chen, Kazuki Munakata},\n",
      "    year={2021},\n",
      "    eprint={TBA},\n",
      "    archivePrefix={TBA},\n",
      "    primaryClass={cs.CL}\n",
      "}\n",
      "```\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 3 documents retrieved which are relevant to the query.\n",
      "Relevant Document #3:\n",
      "Source file: model_cards/Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two/README.md, \n",
      "Content: ---\n",
      "language: en\n",
      "license: apache-2.0\n",
      "datasets:\n",
      "  - hatexplain\n",
      "---\n",
      "\n",
      "\n",
      "## Table of Contents\n",
      "- [Model Details](#model-details)\n",
      "- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n",
      "- [Uses](#uses)\n",
      "- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n",
      "- [Training](#training)\n",
      "- [Evaluation](#evaluation)\n",
      "- [Technical Specifications](#technical-specifications)\n",
      "- [Citation Information](#citation-information)\n",
      "\n",
      "## Model Details\n",
      "**Model Description:** \n",
      "The model is used for classifying a text as Abusive (Hatespeech and Offensive) or Normal. The model is trained using data from Gab and Twitter and Human Rationales were included as part of the training data to boost the performance. The model also has a rationale predictor head that can predict the rationales given an abusive sentence\n",
      "\n",
      "\n",
      "- **Developed by:** Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee \n",
      "- **Model Type:** Text Classification\n",
      "- **Language(s):**  English\n",
      "- **License:**  Apache-2.0\n",
      "- **Parent Model:** See the [BERT base uncased model](https://huggingface.co/bert-base-uncased) for more information about the BERT base model.\n",
      "- **Resources for more information:**\n",
      "  - [Research Paper](https://arxiv.org/abs/2012.10289) Accepted at AAAI 2021.\n",
      "  - [GitHub Repo with datatsets and models](https://github.com/punyajoy/HateXplain)\n",
      "\n",
      "\n",
      "                                                                                               \n",
      "## How to Get Started with the Model\n",
      "\n",
      "**Details of usage**\n",
      "\n",
      "Please use the **Model_Rational_Label** class inside [models.py](models.py) to load the models. The default prediction in this hosted inference API may be wrong due to the use of different class initialisations.\n",
      "\n",
      "```python\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "### from models.py\n",
      "from models import *\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\n",
      "model = Model_Rational_Label.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\n",
      "inputs = tokenizer('He is a great guy\", return_tensors=\"pt\")\n",
      "prediction_logits, _ = model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'])\n",
      "```\n",
      "\n",
      "## Uses\n",
      "\n",
      "#### Direct Use\n",
      "\n",
      "This model can be used for Text Classification\n",
      "\n",
      "\n",
      "#### Downstream Use\n",
      "\n",
      "[More information needed]\n",
      "\n",
      "#### Misuse and Out-of-scope Use\n",
      "\n",
      "The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n",
      "\n",
      "## Risks, Limitations and Biases\n",
      "\n",
      "**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n",
      "\n",
      "Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).\n",
      "\n",
      "(and if you can generate an example of a biased prediction, also something like this): \n",
      "\n",
      "Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For ![example:](https://github.com/hate-alert/HateXplain/blob/master/Figures/dataset_example.png) \n",
      "\n",
      "The model author's also note in their HateXplain paper that they \n",
      ">  *have not considered any external context such as profile bio, user gender, history of posts etc., which might be helpful in the classification task. Also, in this work we have focused on the English language. It does not consider multilingual hate speech into account.*\n",
      "\n",
      "\n",
      "#### Training Procedure\n",
      "\n",
      "##### Preprocessing\n",
      "\n",
      "The authors detail their preprocessing procedure in the [Github repository](https://github.com/hate-alert/HateXplain/tree/master/Preprocess)\n",
      "\n",
      "\n",
      "## Evaluation\n",
      "The mode authors detail the Hidden layer size and attention for the HateXplain fien tuned models in the [associated paper](https://arxiv.org/pdf/2012.10289.pdf)\n",
      "\n",
      "#### Results \n",
      "\n",
      "The model authors both in their paper and in the git repository provide the illustrative output of the BERT - HateXplain in comparison to BERT and and other HateXplain fine tuned ![models]( https://github.com/hate-alert/HateXplain/blob/master/Figures/bias-subgroup.pdf)\n",
      "\n",
      "## Citation Information\n",
      "\n",
      "```bibtex\n",
      "@article{mathew2020hatexplain,\n",
      "  title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},\n",
      "  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},\n",
      "  journal={arXiv preprint arXiv:2012.10289},\n",
      "  year={2020}\n",
      "\n",
      "}\n",
      "```\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 3 documents retrieved which are relevant to the query.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')\n",
    "print(\"*\" * 100)\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"Relevant Document #{i+1}:\\nSource file: {doc.metadata['source']}, \\nContent: {doc.page_content}\")\n",
    "    print(\"-\"*100)\n",
    "    print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be8f3e-fc46-43fa-8a94-b3b4f4575a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5a4d1-aa1f-4520-9991-e4eb9ba1690d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839c15b-b645-4f60-855e-3c765b8f2851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409726e-4ce9-495a-b424-cbf1be5a49e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "715aa867-14e2-4003-aae7-27dd7a74a287",
   "metadata": {},
   "source": [
    "# Step 2: split the best retrieved models into markdown sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "33b43e42-2ee7-48e0-933f-579e52010ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='# Intro  \\n## History  \\nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor.\\nJohn Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  \\nMarkdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.' metadata={'Header 1': 'Intro', 'Header 2': 'History'}\n"
     ]
    }
   ],
   "source": [
    "markdown_document = \"\"\"# Intro \\n\\n    ## History \\n\\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. \n",
    "John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \n",
    "\\n\\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \n",
    "\\n\\n ## Rise and divergence \n",
    "\\n\\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \n",
    "\\n\\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \n",
    "\\n\\n #### Standardization \n",
    "\\n\\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, \n",
    "launched what Atwood characterised as a standardisation effort. \n",
    "\\n\\n ## Implementations \\n\\n Implementations of Markdown are available for over a dozen programming languages.\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "# MD splits\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "print(md_header_splits[0])#.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aac41e-70e0-49bf-81a3-6c9ba109cc01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
