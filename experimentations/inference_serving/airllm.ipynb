{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237db00-b530-409b-9ba6-fef14037080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install memory_profiler airllm bitsandbytes\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d310fb-affb-400a-aa02-957845c19439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> bitsandbytes installed\n",
      ">>>> cache_utils installed\n"
     ]
    }
   ],
   "source": [
    "from airllm import AutoModel"
   ]
  },
  {
   "cell_type": "raw",
   "id": "664873c0-497a-4899-8052-b5197ce9a69c",
   "metadata": {},
   "source": [
    "Run the following command in order to log GPU-compute utilization and GPU-memory consumption.\n",
    "#nvidia-smi --query-gpu=timestamp,name,pci.bus_id,temperature.gpu,utilization.gpu,utilization.memory,memory.used --format=csv -l 1 -f logs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b1eab6-f7a5-4f0d-a437-a77f13c52608",
   "metadata": {},
   "source": [
    "# Model decomposition\n",
    "During inference, the original model will first be decomposed and saved layer-wise. Please ensure there is sufficient disk space in the huggingface cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e33ba33f-ded7-4c9d-8ad5-17dc494fbdaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 48822.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_layers:{'model.embed_tokens.': True, 'model.layers.0.': True, 'model.layers.1.': True, 'model.layers.2.': True, 'model.layers.3.': True, 'model.layers.4.': True, 'model.layers.5.': True, 'model.layers.6.': True, 'model.layers.7.': True, 'model.layers.8.': True, 'model.layers.9.': True, 'model.layers.10.': True, 'model.layers.11.': True, 'model.layers.12.': True, 'model.layers.13.': True, 'model.layers.14.': True, 'model.layers.15.': True, 'model.layers.16.': True, 'model.layers.17.': True, 'model.layers.18.': True, 'model.layers.19.': True, 'model.layers.20.': True, 'model.layers.21.': True, 'model.layers.22.': True, 'model.layers.23.': True, 'model.layers.24.': True, 'model.layers.25.': True, 'model.layers.26.': False, 'model.layers.27.': False, 'model.layers.28.': False, 'model.layers.29.': False, 'model.layers.30.': False, 'model.layers.31.': False, 'model.layers.32.': False, 'model.layers.33.': False, 'model.layers.34.': False, 'model.layers.35.': False, 'model.layers.36.': False, 'model.layers.37.': False, 'model.layers.38.': False, 'model.layers.39.': False, 'model.layers.40.': False, 'model.layers.41.': False, 'model.layers.42.': False, 'model.layers.43.': False, 'model.layers.44.': False, 'model.layers.45.': False, 'model.layers.46.': False, 'model.layers.47.': False, 'model.layers.48.': False, 'model.layers.49.': False, 'model.layers.50.': False, 'model.layers.51.': False, 'model.layers.52.': False, 'model.layers.53.': False, 'model.layers.54.': False, 'model.layers.55.': False, 'model.layers.56.': False, 'model.layers.57.': False, 'model.layers.58.': False, 'model.layers.59.': False, 'model.layers.60.': False, 'model.layers.61.': False, 'model.layers.62.': False, 'model.layers.63.': False, 'model.layers.64.': False, 'model.layers.65.': False, 'model.layers.66.': False, 'model.layers.67.': False, 'model.layers.68.': False, 'model.layers.69.': False, 'model.layers.70.': False, 'model.layers.71.': False, 'model.layers.72.': False, 'model.layers.73.': False, 'model.layers.74.': False, 'model.layers.75.': False, 'model.layers.76.': False, 'model.layers.77.': False, 'model.layers.78.': False, 'model.layers.79.': False, 'model.norm.': False, 'lm_head.': False}\n",
      "some layer splits found, some are not, re-save all layers in case there's some corruptions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                           | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████████▋                                                                                                                                        | 6/83 [00:20<01:46,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████████████████████                                                                                                                             | 12/83 [00:44<01:49,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████████████████████████▋                                                                                                                  | 18/83 [01:06<01:35,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|████████████████████████████████████████▍                                                                                                         | 23/83 [01:26<01:52,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████████████████████████▍                                                                                                  | 27/83 [01:47<02:28,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.26.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████████████████████████████████████████▎                                                                                                | 28/83 [01:51<02:48,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.27.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████████████████████████████████                                                                                               | 29/83 [01:54<02:54,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [02:16<00:00, 136.28s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.28.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████████████████████████████████████▊                                                                                             | 30/83 [04:20<40:45, 46.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.29.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████████████████████████████████████████████▌                                                                                           | 31/83 [04:23<28:46, 33.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.30.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████████████████████████████▎                                                                                         | 32/83 [04:27<20:37, 24.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.31.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████████████████                                                                                        | 33/83 [04:31<15:08, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.32.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████████████████████████████████████████████████▊                                                                                      | 34/83 [04:35<11:26, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.33.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████████████████████████████████████▌                                                                                    | 35/83 [04:39<08:42, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [02:14<00:00, 134.21s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.34.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████████████████████████████████▎                                                                                  | 36/83 [07:03<39:58, 51.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.35.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████████████████████████████████████████████████                                                                                 | 37/83 [07:08<28:20, 36.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.36.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████████████████████████████████████████████████████▊                                                                               | 38/83 [07:12<20:20, 27.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.37.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████████████████████████████████▌                                                                             | 39/83 [07:15<14:41, 20.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.38.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████████████████████████████████████▎                                                                           | 40/83 [07:19<10:51, 15.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.39.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████████████████████████████████████████████████████████████████████████                                                                          | 41/83 [07:22<08:09, 11.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [02:19<00:00, 139.12s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.40.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████████████████████████████████████████████████████████████████▉                                                                        | 42/83 [09:52<36:08, 52.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.41.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████████████████████████████████████████████████▋                                                                      | 43/83 [09:55<25:20, 38.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.42.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████████████████████████████████████████████████████████████████████████████▍                                                                    | 44/83 [09:58<17:55, 27.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.43.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████████████████████████████████████████████████████████████▏                                                                  | 45/83 [10:01<12:49, 20.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.44.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████████████████████████████████████████████████████████████████▉                                                                 | 46/83 [10:06<09:32, 15.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [02:04<00:00, 124.47s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.45.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|██████████████████████████████████████████████████████████████████████████████████▋                                                               | 47/83 [12:21<30:50, 51.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.46.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████████████████████████████████████████████████▍                                                             | 48/83 [12:25<21:41, 37.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.47.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|██████████████████████████████████████████████████████████████████████████████████████▏                                                           | 49/83 [12:29<15:31, 27.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.48.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████████████████████████████▉                                                          | 50/83 [12:33<11:06, 20.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.49.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████████████████████████████████████████████████████████████████████▋                                                        | 51/83 [12:37<08:09, 15.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.50.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|███████████████████████████████████████████████████████████████████████████████████████████▍                                                      | 52/83 [12:41<06:09, 11.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:56<00:00, 116.80s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.51.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 53/83 [14:48<23:12, 46.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.52.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████████████████████████████████████████████████████████████████████████████████████████████▉                                                   | 54/83 [14:51<16:09, 33.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.53.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                 | 55/83 [14:54<11:19, 24.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.54.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                                               | 56/83 [14:56<08:01, 17.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.55.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████████████████████████████████████████████████████████████████████████████▎                                             | 57/83 [15:00<05:56, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.56.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                            | 58/83 [15:04<04:25, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:46<00:00, 106.31s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.57.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                          | 59/83 [17:00<16:54, 42.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.58.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                        | 60/83 [17:04<11:44, 30.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.59.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                      | 61/83 [17:07<08:11, 22.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.60.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████                                     | 62/83 [17:10<05:48, 16.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.61.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                   | 63/83 [17:13<04:14, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.62.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 64/83 [17:17<03:08,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [02:04<00:00, 124.56s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.63.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                               | 65/83 [19:31<14:10, 47.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.64.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                              | 66/83 [19:34<09:37, 33.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.65.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                            | 67/83 [19:37<06:34, 24.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.66.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 68/83 [19:40<04:31, 18.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.67.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                        | 69/83 [19:43<03:10, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:55<00:00, 115.46s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.68.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                      | 70/83 [21:49<10:13, 47.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.69.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                     | 71/83 [21:52<06:47, 33.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.70.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 72/83 [21:55<04:32, 24.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.71.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                 | 73/83 [21:58<03:02, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.72.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 74/83 [22:01<02:02, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.73.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉              | 75/83 [22:03<01:22, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:46<00:00, 106.48s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.74.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋            | 76/83 [24:00<04:55, 42.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.75.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍          | 77/83 [24:03<03:02, 30.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.76.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏        | 78/83 [24:06<01:50, 22.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.77.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉       | 79/83 [24:09<01:05, 16.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.78.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 80/83 [24:12<00:37, 12.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.layers.79.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 82/83 [24:16<00:06,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/model.norm.safetensors\n",
      "Loading shard 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.89s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [24:23<00:00, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model/lm_head.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n",
      "peak memory: 12624.00 MiB, increment: 12086.13 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "\n",
    "# could use hugging face model repo id:\n",
    "model = AutoModel.from_pretrained(\"garage-bAInd/Platypus2-70B-instruct\")\n",
    "\n",
    "# or use model's local path...\n",
    "# model = AutoModel.from_pretrained(\"/root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87\",\n",
    "#                                 compression='4bit',\n",
    "#                                 profiling_mode=True\n",
    "#                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b3058d-ed6c-45b1-9337-5ad939c452a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0):   1%|█▍                                                                                                                         | 1/83 [00:01<01:35,  1.17s/it]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be removed and `position_embeddings` will be mandatory.\n",
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [03:28<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:35<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:54<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:28<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:28<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:13<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:26<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:28<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:24<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the capital of United States?\n",
      "Washington, D.C.</s>\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:55<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:36<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:24<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:00<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:53<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:44<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:53<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:19<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:17<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the capital of United States?\n",
      "Washington, D.C.</s>\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:39<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:35<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:18<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:31<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:33<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:27<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:28<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:30<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:30<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the capital of United States?\n",
      "Washington, D.C.</s>\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:37<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:36<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:31<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:48<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:31<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:12<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:05<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:55<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:14<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the capital of United States?\n",
      "Washington, D.C.</s>\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:26<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:36<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:11<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:10<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:24<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:22<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:34<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:09<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:05<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the capital of United States?\n",
      "Washington, D.C.</s>\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:17<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:27<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:28<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:16<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:29<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:34<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:16<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:50<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:21<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the capital of United States?\n",
      "Washington, D.C.</s>\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:50<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:54<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:14<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:27<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:52<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:58<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:22<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:25<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:03<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the capital of United States?\n",
      "Washington, D.C.</s>\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:38<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:47<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:30<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:50<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:16<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:32<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [01:45<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [03:01<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [02:37<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the capital of United States?\n",
      "Washington, D.C.</s>\n",
      "16min 9s ± 1min 29s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "input_text = [\n",
    "        'What is the capital of United States?'\n",
    "    ]\n",
    "\n",
    "input_tokens = model.tokenizer(input_text,\n",
    "    return_tensors=\"pt\", \n",
    "    return_attention_mask=False, \n",
    "    truncation=True, \n",
    "    max_length=MAX_LENGTH, \n",
    "    padding=False)\n",
    "           \n",
    "generation_output = model.generate(\n",
    "    input_tokens['input_ids'].cuda(), \n",
    "    max_new_tokens=20,\n",
    "    use_cache=True,\n",
    "    return_dict_in_generate=True)\n",
    "\n",
    "output = model.tokenizer.decode(generation_output.sequences[0])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbcdaeb-5506-49ee-9889-2e5071129f0c",
   "metadata": {},
   "source": [
    "## GPU resources utilization visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4493e32-48b8-4084-a954-86a4de6faade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "logs = pd.read_csv(\"logs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aadbf6c6-5ed5-4581-839f-b4b7455ac25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs[[\"empty\",\"memory_size\",\"unit\"]] = logs[\" memory.used [MiB]\"].str.split(\" \", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e0fe22-6399-40a6-9d06-3631253005f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                  object\n",
       " name                      object\n",
       " pci.bus_id                object\n",
       " temperature.gpu            int64\n",
       " utilization.gpu [%]       object\n",
       " utilization.memory [%]    object\n",
       " memory.used [MiB]         object\n",
       "empty                      object\n",
       "memory_size                object\n",
       "unit                       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f9610a9-85fd-4e9f-8258-5ce3367d38de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                  object\n",
       " name                      object\n",
       " pci.bus_id                object\n",
       " temperature.gpu            int64\n",
       " utilization.gpu [%]       object\n",
       " utilization.memory [%]    object\n",
       " memory.used [MiB]         object\n",
       "empty                      object\n",
       "memory_size                 int32\n",
       "unit                       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs[\"memory_size\"] = logs[\"memory_size\"].astype('int32')\n",
    "logs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71780c2c-ff2d-48f3-8087-ad9fa902a2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>pci.bus_id</th>\n",
       "      <th>temperature.gpu</th>\n",
       "      <th>utilization.gpu [%]</th>\n",
       "      <th>utilization.memory [%]</th>\n",
       "      <th>memory.used [MiB]</th>\n",
       "      <th>empty</th>\n",
       "      <th>memory_size</th>\n",
       "      <th>unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024/08/08 10:03:01.040</td>\n",
       "      <td>NVIDIA A40</td>\n",
       "      <td>00000000:D6:00.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0 %</td>\n",
       "      <td>0 %</td>\n",
       "      <td>291 MiB</td>\n",
       "      <td></td>\n",
       "      <td>291</td>\n",
       "      <td>MiB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024/08/08 10:03:02.066</td>\n",
       "      <td>NVIDIA A40</td>\n",
       "      <td>00000000:D6:00.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0 %</td>\n",
       "      <td>0 %</td>\n",
       "      <td>291 MiB</td>\n",
       "      <td></td>\n",
       "      <td>291</td>\n",
       "      <td>MiB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024/08/08 10:03:03.079</td>\n",
       "      <td>NVIDIA A40</td>\n",
       "      <td>00000000:D6:00.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0 %</td>\n",
       "      <td>0 %</td>\n",
       "      <td>291 MiB</td>\n",
       "      <td></td>\n",
       "      <td>291</td>\n",
       "      <td>MiB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024/08/08 10:03:04.097</td>\n",
       "      <td>NVIDIA A40</td>\n",
       "      <td>00000000:D6:00.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0 %</td>\n",
       "      <td>0 %</td>\n",
       "      <td>335 MiB</td>\n",
       "      <td></td>\n",
       "      <td>335</td>\n",
       "      <td>MiB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024/08/08 10:03:05.118</td>\n",
       "      <td>NVIDIA A40</td>\n",
       "      <td>00000000:D6:00.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0 %</td>\n",
       "      <td>0 %</td>\n",
       "      <td>335 MiB</td>\n",
       "      <td></td>\n",
       "      <td>335</td>\n",
       "      <td>MiB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>2024/08/08 10:34:54.456</td>\n",
       "      <td>NVIDIA A40</td>\n",
       "      <td>00000000:D6:00.0</td>\n",
       "      <td>40</td>\n",
       "      <td>100 %</td>\n",
       "      <td>0 %</td>\n",
       "      <td>16371 MiB</td>\n",
       "      <td></td>\n",
       "      <td>16371</td>\n",
       "      <td>MiB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1876</th>\n",
       "      <td>2024/08/08 10:34:58.536</td>\n",
       "      <td>NVIDIA A40</td>\n",
       "      <td>00000000:D6:00.0</td>\n",
       "      <td>40</td>\n",
       "      <td>100 %</td>\n",
       "      <td>0 %</td>\n",
       "      <td>17715 MiB</td>\n",
       "      <td></td>\n",
       "      <td>17715</td>\n",
       "      <td>MiB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>2024/08/08 10:35:02.597</td>\n",
       "      <td>NVIDIA A40</td>\n",
       "      <td>00000000:D6:00.0</td>\n",
       "      <td>40</td>\n",
       "      <td>100 %</td>\n",
       "      <td>1 %</td>\n",
       "      <td>17731 MiB</td>\n",
       "      <td></td>\n",
       "      <td>17731</td>\n",
       "      <td>MiB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>2024/08/08 10:35:00.571</td>\n",
       "      <td>NVIDIA A40</td>\n",
       "      <td>00000000:D6:00.0</td>\n",
       "      <td>40</td>\n",
       "      <td>100 %</td>\n",
       "      <td>0 %</td>\n",
       "      <td>17987 MiB</td>\n",
       "      <td></td>\n",
       "      <td>17987</td>\n",
       "      <td>MiB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871</th>\n",
       "      <td>2024/08/08 10:34:53.438</td>\n",
       "      <td>NVIDIA A40</td>\n",
       "      <td>00000000:D6:00.0</td>\n",
       "      <td>41</td>\n",
       "      <td>100 %</td>\n",
       "      <td>11 %</td>\n",
       "      <td>18007 MiB</td>\n",
       "      <td></td>\n",
       "      <td>18007</td>\n",
       "      <td>MiB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10193 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    timestamp         name         pci.bus_id  \\\n",
       "0     2024/08/08 10:03:01.040   NVIDIA A40   00000000:D6:00.0   \n",
       "1     2024/08/08 10:03:02.066   NVIDIA A40   00000000:D6:00.0   \n",
       "2     2024/08/08 10:03:03.079   NVIDIA A40   00000000:D6:00.0   \n",
       "3     2024/08/08 10:03:04.097   NVIDIA A40   00000000:D6:00.0   \n",
       "4     2024/08/08 10:03:05.118   NVIDIA A40   00000000:D6:00.0   \n",
       "...                       ...          ...                ...   \n",
       "1872  2024/08/08 10:34:54.456   NVIDIA A40   00000000:D6:00.0   \n",
       "1876  2024/08/08 10:34:58.536   NVIDIA A40   00000000:D6:00.0   \n",
       "1880  2024/08/08 10:35:02.597   NVIDIA A40   00000000:D6:00.0   \n",
       "1878  2024/08/08 10:35:00.571   NVIDIA A40   00000000:D6:00.0   \n",
       "1871  2024/08/08 10:34:53.438   NVIDIA A40   00000000:D6:00.0   \n",
       "\n",
       "       temperature.gpu  utilization.gpu [%]  utilization.memory [%]  \\\n",
       "0                   39                  0 %                     0 %   \n",
       "1                   39                  0 %                     0 %   \n",
       "2                   39                  0 %                     0 %   \n",
       "3                   39                  0 %                     0 %   \n",
       "4                   39                  0 %                     0 %   \n",
       "...                ...                  ...                     ...   \n",
       "1872                40                100 %                     0 %   \n",
       "1876                40                100 %                     0 %   \n",
       "1880                40                100 %                     1 %   \n",
       "1878                40                100 %                     0 %   \n",
       "1871                41                100 %                    11 %   \n",
       "\n",
       "      memory.used [MiB] empty  memory_size unit  \n",
       "0               291 MiB                291  MiB  \n",
       "1               291 MiB                291  MiB  \n",
       "2               291 MiB                291  MiB  \n",
       "3               335 MiB                335  MiB  \n",
       "4               335 MiB                335  MiB  \n",
       "...                 ...   ...          ...  ...  \n",
       "1872          16371 MiB              16371  MiB  \n",
       "1876          17715 MiB              17715  MiB  \n",
       "1880          17731 MiB              17731  MiB  \n",
       "1878          17987 MiB              17987  MiB  \n",
       "1871          18007 MiB              18007  MiB  \n",
       "\n",
       "[10193 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.sort_values(by=[\"memory_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c1f38-7bb8-450c-aaf0-95a38d21f367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca949d1-f5eb-4457-ba8a-bacaf2ccaf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use model's local path...\n",
    "model = AutoModel.from_pretrained(\"/root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87\",\n",
    "                                 compression='4bit',\n",
    "                                 profiling_mode=True\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac49e73-f37a-41a4-9ce2-2e7bcdc37360",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "input_text = [\n",
    "    'What is the capital of Italy?'\n",
    "]\n",
    "\n",
    "input_tokens = model.tokenizer(input_text,\n",
    "    return_tensors=\"pt\", \n",
    "    return_attention_mask=False, \n",
    "    truncation=True, \n",
    "    max_length=MAX_LENGTH, \n",
    "    padding=False)\n",
    "           \n",
    "generation_output = model.generate(\n",
    "    input_tokens['input_ids'].cuda(), \n",
    "    max_new_tokens=20,\n",
    "    use_cache=True,\n",
    "    return_dict_in_generate=True)\n",
    "\n",
    "output = model.tokenizer.decode(generation_output.sequences[0])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0691a35-8765-44a0-bc32-11e0ed95a680",
   "metadata": {},
   "source": [
    "# Model v2ray/Llama-3-70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621f923d-20de-440c-93b6-0dd028b75904",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.27it/s]\n",
      "  0%|                                                                                                                                                           | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [08:03<00:00, 483.94s/it]\u001b[A\n",
      "  1%|█▋                                                                                                                                             | 1/83 [08:05<11:03:51, 485.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.embed_tokens.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▍                                                                                                                                            | 2/83 [08:07<4:31:06, 200.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.0.safetensors\n",
      "Loading shard 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:57<00:00, 57.10s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.1.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████▉                                                                                                                                          | 4/83 [09:07<1:49:01, 82.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.2.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████████▋                                                                                                                                        | 5/83 [09:08<1:09:24, 53.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.3.safetensors\n",
      "Loading shard 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:58<00:00, 58.13s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.4.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|████████████▍                                                                                                                                      | 7/83 [10:09<47:53, 37.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.5.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▏                                                                                                                                    | 8/83 [10:10<32:43, 26.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.6.safetensors\n",
      "Loading shard 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:53<00:00, 53.40s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.7.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▌                                                                                                                                | 10/83 [11:07<30:04, 24.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.8.safetensors\n",
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.9.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████████████▎                                                                                                                              | 11/83 [11:08<21:09, 17.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:50<00:00, 50.66s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.10.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████████████▊                                                                                                                           | 13/83 [12:02<23:21, 20.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.11.safetensors\n",
      "Loading shard 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:51<00:00, 51.36s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.12.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████████████████▍                                                                                                                       | 15/83 [12:56<24:15, 21.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.13.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████████████████████████████▏                                                                                                                     | 16/83 [12:58<17:11, 15.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.14.safetensors\n",
      "Loading shard 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:58<00:00, 58.35s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.15.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████████████████████████▋                                                                                                                  | 18/83 [13:59<22:18, 20.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.16.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████████████████████████████▍                                                                                                                | 19/83 [14:01<15:47, 14.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.17.safetensors\n",
      "Loading shard 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:02<00:00, 62.69s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.18.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████████████████████▉                                                                                                             | 21/83 [15:07<21:57, 21.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.19.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████████████████████▋                                                                                                           | 22/83 [15:08<15:31, 15.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.20.safetensors\n",
      "Loading shard 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:54<00:00, 54.18s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.21.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████████████████████████████████████▏                                                                                                       | 24/83 [16:05<19:16, 19.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.22.safetensors\n",
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.23.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████████████████▉                                                                                                      | 25/83 [16:07<13:42, 14.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:57<00:00, 57.57s/it]\u001b[A\n",
      " 31%|█████████████████████████████████████████████▋                                                                                                    | 26/83 [17:06<26:16, 27.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.24.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████████████████████████▍                                                                                                  | 27/83 [17:07<18:25, 19.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.25.safetensors\n",
      "Loading shard 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:50<00:00, 50.06s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.26.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████████████████████████████████                                                                                               | 29/83 [18:00<18:52, 20.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.27.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████████████████████████████████████▊                                                                                             | 30/83 [18:02<13:20, 15.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.28.safetensors\n",
      "Loading shard 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:49<00:00, 49.60s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.29.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████████████████████████████▎                                                                                         | 32/83 [18:54<15:47, 18.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.30.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████████████████                                                                                        | 33/83 [18:56<11:12, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.31.safetensors\n",
      "Loading shard 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:11<00:00, 71.16s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.32.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████████████████████████████████████▌                                                                                    | 35/83 [20:10<17:52, 22.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.33.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████████████████████████████████▎                                                                                  | 36/83 [20:12<12:34, 16.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.34.safetensors\n",
      "Loading shard 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:05<00:00, 65.17s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.35.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████████████████████████████████████████████████████▊                                                                               | 38/83 [21:20<16:45, 22.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.36.safetensors\n",
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.37.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████████████████████████████████▌                                                                             | 39/83 [21:22<11:49, 16.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:54<00:00, 54.41s/it]\u001b[A\n",
      " 48%|██████████████████████████████████████████████████████████████████████▎                                                                           | 40/83 [22:17<20:06, 28.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.38.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████████████████████████████████████████████████████████████████████████                                                                          | 41/83 [22:19<14:01, 20.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.39.safetensors\n",
      "Loading shard 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:54<00:00, 54.40s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.40.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████████████████████████████████████████████████▋                                                                      | 43/83 [23:16<14:41, 22.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.41.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████████████████████████████████████████████████████████████████████████████▍                                                                    | 44/83 [23:18<10:17, 15.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.42.safetensors\n",
      "Loading shard 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:06<00:00, 66.07s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.43.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████████████████████████████████████████████████████████████████▉                                                                 | 46/83 [24:27<13:50, 22.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.44.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|██████████████████████████████████████████████████████████████████████████████████▋                                                               | 47/83 [24:28<09:39, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.45.safetensors\n",
      "Loading shard 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:56<00:00, 56.41s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.46.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|██████████████████████████████████████████████████████████████████████████████████████▏                                                           | 49/83 [25:28<11:36, 20.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.47.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████████████████████████████▉                                                          | 50/83 [25:29<08:05, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.48.safetensors\n",
      "Loading shard 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:54<00:00, 54.13s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.49.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|███████████████████████████████████████████████████████████████████████████████████████████▍                                                      | 52/83 [26:26<09:59, 19.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.50.safetensors\n",
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.51.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 53/83 [26:28<06:59, 13.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:51<00:00, 51.37s/it]\u001b[A\n",
      " 65%|██████████████████████████████████████████████████████████████████████████████████████████████▉                                                   | 54/83 [27:21<12:23, 25.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.52.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                 | 55/83 [27:22<08:34, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.53.safetensors\n",
      "Loading shard 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:12<00:00, 72.01s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.54.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████████████████████████████████████████████████████████████████████████████▎                                             | 57/83 [28:37<10:47, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.55.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                            | 58/83 [28:38<07:25, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.56.safetensors\n",
      "Loading shard 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:55<00:00, 55.16s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.57.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                        | 60/83 [29:37<08:05, 21.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.58.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                      | 61/83 [29:38<05:35, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.59.safetensors\n",
      "Loading shard 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:57<00:00, 57.11s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.60.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                   | 63/83 [30:39<06:46, 20.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.61.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 64/83 [30:40<04:38, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.62.safetensors\n",
      "Loading shard 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:59<00:00, 59.39s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.63.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                              | 66/83 [31:43<05:48, 20.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.64.safetensors\n",
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.65.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                            | 67/83 [31:45<03:57, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:51<00:00, 51.82s/it]\u001b[A\n",
      " 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 68/83 [32:38<06:36, 26.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.66.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                        | 69/83 [32:40<04:25, 18.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.67.safetensors\n",
      "Loading shard 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [08:01<00:00, 481.79s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.68.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 71/83 [40:45<22:15, 111.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.69.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 72/83 [40:46<14:21, 78.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.70.safetensors\n",
      "Loading shard 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:56<00:00, 56.76s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.71.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 74/83 [41:46<07:40, 51.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.72.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉              | 75/83 [41:48<04:50, 36.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.73.safetensors\n",
      "Loading shard 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:03<00:00, 63.45s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.74.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍          | 77/83 [42:55<03:12, 32.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.75.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏        | 78/83 [42:57<01:54, 22.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.76.safetensors\n",
      "Loading shard 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:05<00:00, 65.25s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.77.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 80/83 [44:05<01:17, 25.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.78.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 81/83 [44:07<00:36, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.layers.79.safetensors\n",
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/model.norm.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 82/83 [44:07<00:13, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching 1 files:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:25<00:00, 25.37s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as: /root/.cache/huggingface/hub/models--v2ray--Llama-3-70B/snapshots/cfc88c331bbe5371d8ffc3477878ee22793c2463/splitted_model.4bit/lm_head.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [44:34<00:00, 32.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n",
      "not support prefetching for compression for now. loading with no prepetching mode.\n"
     ]
    }
   ],
   "source": [
    "model2 = AutoModel.from_pretrained(\"v2ray/Llama-3-70B\",\n",
    "                                  compression=\"4bit\",\n",
    "                                  profiling_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e6c1f-5e95-4733-afd8-a31082c9b3d4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:51<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 24.560622067944678\n",
      "total time for compression_time: 15.20098557899746\n",
      "total time for create_layer_from_safe_tensor: 0.046357154846191406\n",
      "total infer process time(including all above plus gpu compute): 31.1619\n",
      "total infer wall time(including all above plus gpu compute): 52.3754\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:26<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 4.145547471430291\n",
      "total time for compression_time: 9.92665687999488\n",
      "total time for create_layer_from_safe_tensor: 0.047362327575683594\n",
      "total infer process time(including all above plus gpu compute): 26.4633\n",
      "total infer wall time(including all above plus gpu compute): 26.9039\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.9096153607906672\n",
      "total time for compression_time: 9.951509869998517\n",
      "total time for create_layer_from_safe_tensor: 0.04826545715332031\n",
      "total infer process time(including all above plus gpu compute): 24.9694\n",
      "total infer wall time(including all above plus gpu compute): 24.5755\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.553022691701699\n",
      "total time for compression_time: 9.977535656000327\n",
      "total time for create_layer_from_safe_tensor: 0.04343867301940918\n",
      "total infer process time(including all above plus gpu compute): 24.2078\n",
      "total infer wall time(including all above plus gpu compute): 23.6024\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.3437853264294972\n",
      "total time for compression_time: 9.897396524003852\n",
      "total time for create_layer_from_safe_tensor: 0.04666566848754883\n",
      "total infer process time(including all above plus gpu compute): 24.0396\n",
      "total infer wall time(including all above plus gpu compute): 23.4162\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4428980556403985\n",
      "total time for compression_time: 10.076058844002546\n",
      "total time for create_layer_from_safe_tensor: 0.04433417320251465\n",
      "total infer process time(including all above plus gpu compute): 24.6166\n",
      "total infer wall time(including all above plus gpu compute): 24.1527\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5449416226010726\n",
      "total time for compression_time: 10.084642213000734\n",
      "total time for create_layer_from_safe_tensor: 0.04385733604431152\n",
      "total infer process time(including all above plus gpu compute): 24.8085\n",
      "total infer wall time(including all above plus gpu compute): 24.2862\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4087336024331307\n",
      "total time for compression_time: 9.903810505004003\n",
      "total time for create_layer_from_safe_tensor: 0.04306197166442871\n",
      "total infer process time(including all above plus gpu compute): 24.1671\n",
      "total infer wall time(including all above plus gpu compute): 23.6326\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4413869524141774\n",
      "total time for compression_time: 10.027850851998664\n",
      "total time for create_layer_from_safe_tensor: 0.04372048377990723\n",
      "total infer process time(including all above plus gpu compute): 23.9858\n",
      "total infer wall time(including all above plus gpu compute): 23.4755\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.660576756370574\n",
      "total time for compression_time: 9.749427620994538\n",
      "total time for create_layer_from_safe_tensor: 0.043230533599853516\n",
      "total infer process time(including all above plus gpu compute): 24.4708\n",
      "total infer wall time(including all above plus gpu compute): 23.7925\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.492300863039418\n",
      "total time for compression_time: 9.924395447003917\n",
      "total time for create_layer_from_safe_tensor: 0.04568624496459961\n",
      "total infer process time(including all above plus gpu compute): 25.2637\n",
      "total infer wall time(including all above plus gpu compute): 24.6471\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.28993765080304\n",
      "total time for compression_time: 9.946044052001525\n",
      "total time for create_layer_from_safe_tensor: 0.041736602783203125\n",
      "total infer process time(including all above plus gpu compute): 23.8078\n",
      "total infer wall time(including all above plus gpu compute): 23.3663\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6346995042285926\n",
      "total time for compression_time: 9.647898275999069\n",
      "total time for create_layer_from_safe_tensor: 0.042282819747924805\n",
      "total infer process time(including all above plus gpu compute): 23.9916\n",
      "total infer wall time(including all above plus gpu compute): 23.1716\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5602464307494301\n",
      "total time for compression_time: 9.739630735998617\n",
      "total time for create_layer_from_safe_tensor: 0.041934967041015625\n",
      "total infer process time(including all above plus gpu compute): 23.8342\n",
      "total infer wall time(including all above plus gpu compute): 23.5545\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4676848958333721\n",
      "total time for compression_time: 10.245444863001467\n",
      "total time for create_layer_from_safe_tensor: 0.049273014068603516\n",
      "total infer process time(including all above plus gpu compute): 25.0165\n",
      "total infer wall time(including all above plus gpu compute): 24.6801\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5285232478718171\n",
      "total time for compression_time: 9.960095125999032\n",
      "total time for create_layer_from_safe_tensor: 0.04446053504943848\n",
      "total infer process time(including all above plus gpu compute): 25.1821\n",
      "total infer wall time(including all above plus gpu compute): 24.1799\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.555864614145321\n",
      "total time for compression_time: 9.929952103002506\n",
      "total time for create_layer_from_safe_tensor: 0.04756760597229004\n",
      "total infer process time(including all above plus gpu compute): 24.5237\n",
      "total infer wall time(including all above plus gpu compute): 24.1719\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4440908519554796\n",
      "total time for compression_time: 9.983399144000941\n",
      "total time for create_layer_from_safe_tensor: 0.04554104804992676\n",
      "total infer process time(including all above plus gpu compute): 24.4985\n",
      "total infer wall time(including all above plus gpu compute): 24.0711\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.652929655254411\n",
      "total time for compression_time: 9.962751992999983\n",
      "total time for create_layer_from_safe_tensor: 0.0462651252746582\n",
      "total infer process time(including all above plus gpu compute): 24.2609\n",
      "total infer wall time(including all above plus gpu compute): 24.0176\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:25<00:00,  3.26it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5067993925003975\n",
      "total time for compression_time: 10.00697643300009\n",
      "total time for create_layer_from_safe_tensor: 0.044506072998046875\n",
      "total infer process time(including all above plus gpu compute): 24.5715\n",
      "total infer wall time(including all above plus gpu compute): 26.3287\n",
      "<|begin_of_text|>What is the capital of France? The capital of France is Paris. Where is France located? What is the climate of France? What\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6679168883310922\n",
      "total time for compression_time: 9.848985319997155\n",
      "total time for create_layer_from_safe_tensor: 0.04519391059875488\n",
      "total infer process time(including all above plus gpu compute): 25.2781\n",
      "total infer wall time(including all above plus gpu compute): 23.9936\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5363612179717165\n",
      "total time for compression_time: 9.931792973999109\n",
      "total time for create_layer_from_safe_tensor: 0.04622960090637207\n",
      "total infer process time(including all above plus gpu compute): 24.3731\n",
      "total infer wall time(including all above plus gpu compute): 24.0300\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5283907649054527\n",
      "total time for compression_time: 9.849890589998722\n",
      "total time for create_layer_from_safe_tensor: 0.04727292060852051\n",
      "total infer process time(including all above plus gpu compute): 24.1190\n",
      "total infer wall time(including all above plus gpu compute): 23.7998\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4938283928086094\n",
      "total time for compression_time: 9.942660473997421\n",
      "total time for create_layer_from_safe_tensor: 0.04677295684814453\n",
      "total infer process time(including all above plus gpu compute): 24.5823\n",
      "total infer wall time(including all above plus gpu compute): 24.2134\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5407841668711626\n",
      "total time for compression_time: 9.844932033003715\n",
      "total time for create_layer_from_safe_tensor: 0.043743133544921875\n",
      "total infer process time(including all above plus gpu compute): 24.7029\n",
      "total infer wall time(including all above plus gpu compute): 24.1503\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4823213407798903\n",
      "total time for compression_time: 9.934653919000993\n",
      "total time for create_layer_from_safe_tensor: 0.043097496032714844\n",
      "total infer process time(including all above plus gpu compute): 24.2939\n",
      "total infer wall time(including all above plus gpu compute): 23.8922\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5964060354908725\n",
      "total time for compression_time: 10.066792673997043\n",
      "total time for create_layer_from_safe_tensor: 0.04523634910583496\n",
      "total infer process time(including all above plus gpu compute): 24.5179\n",
      "total infer wall time(including all above plus gpu compute): 24.2502\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4705743984459332\n",
      "total time for compression_time: 10.147706965994985\n",
      "total time for create_layer_from_safe_tensor: 0.045854806900024414\n",
      "total infer process time(including all above plus gpu compute): 25.2776\n",
      "total infer wall time(including all above plus gpu compute): 24.4443\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.770716901504784\n",
      "total time for compression_time: 9.705710891998024\n",
      "total time for create_layer_from_safe_tensor: 0.04417586326599121\n",
      "total infer process time(including all above plus gpu compute): 25.0839\n",
      "total infer wall time(including all above plus gpu compute): 23.9231\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6265895355736575\n",
      "total time for compression_time: 9.812468053003613\n",
      "total time for create_layer_from_safe_tensor: 0.043589115142822266\n",
      "total infer process time(including all above plus gpu compute): 24.4360\n",
      "total infer wall time(including all above plus gpu compute): 24.1812\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6100077564724415\n",
      "total time for compression_time: 9.527933604001191\n",
      "total time for create_layer_from_safe_tensor: 0.04267239570617676\n",
      "total infer process time(including all above plus gpu compute): 23.5857\n",
      "total infer wall time(including all above plus gpu compute): 23.5141\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.7063599045877709\n",
      "total time for compression_time: 9.78754182499597\n",
      "total time for create_layer_from_safe_tensor: 0.04508805274963379\n",
      "total infer process time(including all above plus gpu compute): 25.0113\n",
      "total infer wall time(including all above plus gpu compute): 23.7676\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6724888709932202\n",
      "total time for compression_time: 9.823550757989324\n",
      "total time for create_layer_from_safe_tensor: 0.04499626159667969\n",
      "total infer process time(including all above plus gpu compute): 23.9142\n",
      "total infer wall time(including all above plus gpu compute): 23.7253\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4657215756478763\n",
      "total time for compression_time: 9.747457058995678\n",
      "total time for create_layer_from_safe_tensor: 0.04362654685974121\n",
      "total infer process time(including all above plus gpu compute): 24.2016\n",
      "total infer wall time(including all above plus gpu compute): 23.6816\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6729048626539225\n",
      "total time for compression_time: 9.848952399003792\n",
      "total time for create_layer_from_safe_tensor: 0.04432392120361328\n",
      "total infer process time(including all above plus gpu compute): 24.1778\n",
      "total infer wall time(including all above plus gpu compute): 23.9347\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.8235753330081934\n",
      "total time for compression_time: 9.656572744002915\n",
      "total time for create_layer_from_safe_tensor: 0.04478716850280762\n",
      "total infer process time(including all above plus gpu compute): 25.0835\n",
      "total infer wall time(including all above plus gpu compute): 23.8789\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.492932435815419\n",
      "total time for compression_time: 9.790476205999767\n",
      "total time for create_layer_from_safe_tensor: 0.04415726661682129\n",
      "total infer process time(including all above plus gpu compute): 24.2337\n",
      "total infer wall time(including all above plus gpu compute): 23.5204\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.623535195813929\n",
      "total time for compression_time: 9.853430470003332\n",
      "total time for create_layer_from_safe_tensor: 0.04415154457092285\n",
      "total infer process time(including all above plus gpu compute): 25.2729\n",
      "total infer wall time(including all above plus gpu compute): 24.1557\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6169789468058298\n",
      "total time for compression_time: 10.04692452800191\n",
      "total time for create_layer_from_safe_tensor: 0.04547309875488281\n",
      "total infer process time(including all above plus gpu compute): 24.3990\n",
      "total infer wall time(including all above plus gpu compute): 24.1060\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.55it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6099065810994944\n",
      "total time for compression_time: 9.90461859399511\n",
      "total time for create_layer_from_safe_tensor: 0.04614377021789551\n",
      "total infer process time(including all above plus gpu compute): 25.5379\n",
      "total infer wall time(including all above plus gpu compute): 24.3184\n",
      "<|begin_of_text|>What is the capital of France? What is the capital of the UK?\n",
      "What is the capital of France? What is the capital of\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5439976013667547\n",
      "total time for compression_time: 9.872220441006903\n",
      "total time for create_layer_from_safe_tensor: 0.04158949851989746\n",
      "total infer process time(including all above plus gpu compute): 23.8765\n",
      "total infer wall time(including all above plus gpu compute): 23.4688\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5406939852828145\n",
      "total time for compression_time: 9.567989124000633\n",
      "total time for create_layer_from_safe_tensor: 0.04139590263366699\n",
      "total infer process time(including all above plus gpu compute): 24.5761\n",
      "total infer wall time(including all above plus gpu compute): 23.2860\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5896673704264686\n",
      "total time for compression_time: 9.67939705599565\n",
      "total time for create_layer_from_safe_tensor: 0.04290938377380371\n",
      "total infer process time(including all above plus gpu compute): 23.8231\n",
      "total infer wall time(including all above plus gpu compute): 23.4745\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4974549758098874\n",
      "total time for compression_time: 9.857646847996875\n",
      "total time for create_layer_from_safe_tensor: 0.043015241622924805\n",
      "total infer process time(including all above plus gpu compute): 24.0587\n",
      "total infer wall time(including all above plus gpu compute): 23.7405\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5398193278688268\n",
      "total time for compression_time: 9.84815016599714\n",
      "total time for create_layer_from_safe_tensor: 0.042730093002319336\n",
      "total infer process time(including all above plus gpu compute): 24.0013\n",
      "total infer wall time(including all above plus gpu compute): 23.7328\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.3543212619979386\n",
      "total time for compression_time: 9.935177544001817\n",
      "total time for create_layer_from_safe_tensor: 0.04246950149536133\n",
      "total infer process time(including all above plus gpu compute): 23.7373\n",
      "total infer wall time(including all above plus gpu compute): 23.3520\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.7014581002049454\n",
      "total time for compression_time: 9.581968423003673\n",
      "total time for create_layer_from_safe_tensor: 0.04357314109802246\n",
      "total infer process time(including all above plus gpu compute): 24.4192\n",
      "total infer wall time(including all above plus gpu compute): 23.3239\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5653093954988435\n",
      "total time for compression_time: 9.756446728997616\n",
      "total time for create_layer_from_safe_tensor: 0.04259991645812988\n",
      "total infer process time(including all above plus gpu compute): 24.4582\n",
      "total infer wall time(including all above plus gpu compute): 23.8848\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.551998706703671\n",
      "total time for compression_time: 9.963111786002628\n",
      "total time for create_layer_from_safe_tensor: 0.04556894302368164\n",
      "total infer process time(including all above plus gpu compute): 24.8111\n",
      "total infer wall time(including all above plus gpu compute): 24.1895\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.522860384267915\n",
      "total time for compression_time: 9.81813921900357\n",
      "total time for create_layer_from_safe_tensor: 0.0442202091217041\n",
      "total infer process time(including all above plus gpu compute): 24.2565\n",
      "total infer wall time(including all above plus gpu compute): 23.8900\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.3709863916064933\n",
      "total time for compression_time: 9.774222443995313\n",
      "total time for create_layer_from_safe_tensor: 0.04330730438232422\n",
      "total infer process time(including all above plus gpu compute): 23.7457\n",
      "total infer wall time(including all above plus gpu compute): 23.4063\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4985446091304766\n",
      "total time for compression_time: 10.010904395996477\n",
      "total time for create_layer_from_safe_tensor: 0.04635977745056152\n",
      "total infer process time(including all above plus gpu compute): 24.6853\n",
      "total infer wall time(including all above plus gpu compute): 24.0615\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4353911929083552\n",
      "total time for compression_time: 9.852610105996064\n",
      "total time for create_layer_from_safe_tensor: 0.04040861129760742\n",
      "total infer process time(including all above plus gpu compute): 23.6907\n",
      "total infer wall time(including all above plus gpu compute): 23.2817\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.3776897077113972\n",
      "total time for compression_time: 9.89092148499185\n",
      "total time for create_layer_from_safe_tensor: 0.0422968864440918\n",
      "total infer process time(including all above plus gpu compute): 24.2149\n",
      "total infer wall time(including all above plus gpu compute): 23.6638\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.265257970422681\n",
      "total time for compression_time: 10.09702788200957\n",
      "total time for create_layer_from_safe_tensor: 0.040496826171875\n",
      "total infer process time(including all above plus gpu compute): 23.7708\n",
      "total infer wall time(including all above plus gpu compute): 23.0941\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6404648993484443\n",
      "total time for compression_time: 9.762037399006658\n",
      "total time for create_layer_from_safe_tensor: 0.03860664367675781\n",
      "total infer process time(including all above plus gpu compute): 24.2637\n",
      "total infer wall time(including all above plus gpu compute): 23.0664\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:24<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5531116215915972\n",
      "total time for compression_time: 9.747369220998735\n",
      "total time for create_layer_from_safe_tensor: 0.042121171951293945\n",
      "total infer process time(including all above plus gpu compute): 23.9764\n",
      "total infer wall time(including all above plus gpu compute): 25.1408\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.616826364695953\n",
      "total time for compression_time: 9.778414418995453\n",
      "total time for create_layer_from_safe_tensor: 0.042745351791381836\n",
      "total infer process time(including all above plus gpu compute): 24.9840\n",
      "total infer wall time(including all above plus gpu compute): 23.9115\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.644561508197512\n",
      "total time for compression_time: 9.877975007992063\n",
      "total time for create_layer_from_safe_tensor: 0.04469895362854004\n",
      "total infer process time(including all above plus gpu compute): 24.3516\n",
      "total infer wall time(including all above plus gpu compute): 23.9406\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6750493139934406\n",
      "total time for compression_time: 9.644629231004728\n",
      "total time for create_layer_from_safe_tensor: 0.04153251647949219\n",
      "total infer process time(including all above plus gpu compute): 23.7566\n",
      "total infer wall time(including all above plus gpu compute): 23.4675\n",
      "<|begin_of_text|>What is the capital of France?What is the name of the river in Paris?What are the nicknames of Paris?What is\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5790680932987016\n",
      "total time for compression_time: 9.70613560199854\n",
      "total time for create_layer_from_safe_tensor: 0.040017127990722656\n",
      "total infer process time(including all above plus gpu compute): 23.7892\n",
      "total infer wall time(including all above plus gpu compute): 23.2394\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.3492833033069473\n",
      "total time for compression_time: 9.808585082007994\n",
      "total time for create_layer_from_safe_tensor: 0.039067983627319336\n",
      "total infer process time(including all above plus gpu compute): 23.6450\n",
      "total infer wall time(including all above plus gpu compute): 23.2777\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5979774611005269\n",
      "total time for compression_time: 9.623193917988829\n",
      "total time for create_layer_from_safe_tensor: 0.04121828079223633\n",
      "total infer process time(including all above plus gpu compute): 24.5638\n",
      "total infer wall time(including all above plus gpu compute): 23.3170\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.7788724449619622\n",
      "total time for compression_time: 9.648170992995801\n",
      "total time for create_layer_from_safe_tensor: 0.04249382019042969\n",
      "total infer process time(including all above plus gpu compute): 24.8155\n",
      "total infer wall time(including all above plus gpu compute): 23.5857\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5383802546693914\n",
      "total time for compression_time: 9.67540444000224\n",
      "total time for create_layer_from_safe_tensor: 0.04061150550842285\n",
      "total infer process time(including all above plus gpu compute): 23.9756\n",
      "total infer wall time(including all above plus gpu compute): 23.2551\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.638193146131016\n",
      "total time for compression_time: 9.605589851000332\n",
      "total time for create_layer_from_safe_tensor: 0.04033255577087402\n",
      "total infer process time(including all above plus gpu compute): 24.5268\n",
      "total infer wall time(including all above plus gpu compute): 23.2251\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5874929886504106\n",
      "total time for compression_time: 9.898512556012065\n",
      "total time for create_layer_from_safe_tensor: 0.04456663131713867\n",
      "total infer process time(including all above plus gpu compute): 24.7827\n",
      "total infer wall time(including all above plus gpu compute): 23.7712\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.663564429833059\n",
      "total time for compression_time: 9.875488295005198\n",
      "total time for create_layer_from_safe_tensor: 0.04277992248535156\n",
      "total infer process time(including all above plus gpu compute): 25.1582\n",
      "total infer wall time(including all above plus gpu compute): 24.1053\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.478320284102665\n",
      "total time for compression_time: 9.914364414002193\n",
      "total time for create_layer_from_safe_tensor: 0.04076051712036133\n",
      "total infer process time(including all above plus gpu compute): 23.7621\n",
      "total infer wall time(including all above plus gpu compute): 23.3303\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.504654162325096\n",
      "total time for compression_time: 9.688060051999855\n",
      "total time for create_layer_from_safe_tensor: 0.039020538330078125\n",
      "total infer process time(including all above plus gpu compute): 23.6502\n",
      "total infer wall time(including all above plus gpu compute): 23.1995\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:24<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6242665258105262\n",
      "total time for compression_time: 9.737911323005392\n",
      "total time for create_layer_from_safe_tensor: 0.04403972625732422\n",
      "total infer process time(including all above plus gpu compute): 26.2852\n",
      "total infer wall time(including all above plus gpu compute): 24.9828\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.688307607833849\n",
      "total time for compression_time: 9.924537813003553\n",
      "total time for create_layer_from_safe_tensor: 0.04446125030517578\n",
      "total infer process time(including all above plus gpu compute): 24.2902\n",
      "total infer wall time(including all above plus gpu compute): 24.0225\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6078731618799793\n",
      "total time for compression_time: 9.851345817001857\n",
      "total time for create_layer_from_safe_tensor: 0.040845632553100586\n",
      "total infer process time(including all above plus gpu compute): 24.9339\n",
      "total infer wall time(including all above plus gpu compute): 23.6469\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.629231000370055\n",
      "total time for compression_time: 9.651525950008363\n",
      "total time for create_layer_from_safe_tensor: 0.040261030197143555\n",
      "total infer process time(including all above plus gpu compute): 24.2099\n",
      "total infer wall time(including all above plus gpu compute): 23.4337\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5775148743723548\n",
      "total time for compression_time: 9.829643261995443\n",
      "total time for create_layer_from_safe_tensor: 0.04109835624694824\n",
      "total infer process time(including all above plus gpu compute): 23.7679\n",
      "total infer wall time(including all above plus gpu compute): 23.2944\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4645052503692568\n",
      "total time for compression_time: 9.82631749400025\n",
      "total time for create_layer_from_safe_tensor: 0.0414886474609375\n",
      "total infer process time(including all above plus gpu compute): 24.7514\n",
      "total infer wall time(including all above plus gpu compute): 23.6918\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5325442828379892\n",
      "total time for compression_time: 9.911262126997826\n",
      "total time for create_layer_from_safe_tensor: 0.04133009910583496\n",
      "total infer process time(including all above plus gpu compute): 24.4078\n",
      "total infer wall time(including all above plus gpu compute): 23.7751\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5924441210281657\n",
      "total time for compression_time: 9.761428655003328\n",
      "total time for create_layer_from_safe_tensor: 0.04005169868469238\n",
      "total infer process time(including all above plus gpu compute): 24.6966\n",
      "total infer wall time(including all above plus gpu compute): 23.5276\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5570948228469206\n",
      "total time for compression_time: 9.740550746000736\n",
      "total time for create_layer_from_safe_tensor: 0.04044651985168457\n",
      "total infer process time(including all above plus gpu compute): 24.2852\n",
      "total infer wall time(including all above plus gpu compute): 23.1439\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.492649040894321\n",
      "total time for compression_time: 9.9254415409996\n",
      "total time for create_layer_from_safe_tensor: 0.03871560096740723\n",
      "total infer process time(including all above plus gpu compute): 23.9274\n",
      "total infer wall time(including all above plus gpu compute): 23.5025\n",
      "<|begin_of_text|>What is the capital of France? Paris! What is the capital of Norway? Oslo! What is the capital of Belarus? Minsk\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:26<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.7795395950197417\n",
      "total time for compression_time: 10.605675449001865\n",
      "total time for create_layer_from_safe_tensor: 0.04782533645629883\n",
      "total infer process time(including all above plus gpu compute): 27.7958\n",
      "total infer wall time(including all above plus gpu compute): 27.4565\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5299125888377603\n",
      "total time for compression_time: 10.066799762007577\n",
      "total time for create_layer_from_safe_tensor: 0.04286026954650879\n",
      "total infer process time(including all above plus gpu compute): 24.4017\n",
      "total infer wall time(including all above plus gpu compute): 24.0775\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5666552064467396\n",
      "total time for compression_time: 9.953076791996864\n",
      "total time for create_layer_from_safe_tensor: 0.0413508415222168\n",
      "total infer process time(including all above plus gpu compute): 23.9433\n",
      "total infer wall time(including all above plus gpu compute): 23.6296\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4524341203632503\n",
      "total time for compression_time: 9.873347225004181\n",
      "total time for create_layer_from_safe_tensor: 0.040311336517333984\n",
      "total infer process time(including all above plus gpu compute): 23.8449\n",
      "total infer wall time(including all above plus gpu compute): 23.2812\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:21<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.3786966390980524\n",
      "total time for compression_time: 9.63870839401352\n",
      "total time for create_layer_from_safe_tensor: 0.038333892822265625\n",
      "total infer process time(including all above plus gpu compute): 23.1756\n",
      "total infer wall time(including all above plus gpu compute): 22.7748\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.279521809939979\n",
      "total time for compression_time: 10.186915053005578\n",
      "total time for create_layer_from_safe_tensor: 0.043645381927490234\n",
      "total infer process time(including all above plus gpu compute): 24.9871\n",
      "total infer wall time(including all above plus gpu compute): 24.0862\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.336380912194727\n",
      "total time for compression_time: 10.104404018988134\n",
      "total time for create_layer_from_safe_tensor: 0.042946577072143555\n",
      "total infer process time(including all above plus gpu compute): 24.1447\n",
      "total infer wall time(including all above plus gpu compute): 23.5886\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.3781731657090859\n",
      "total time for compression_time: 10.17831749400466\n",
      "total time for create_layer_from_safe_tensor: 0.0407106876373291\n",
      "total infer process time(including all above plus gpu compute): 24.1890\n",
      "total infer wall time(including all above plus gpu compute): 23.6698\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.3973093247623183\n",
      "total time for compression_time: 9.904159286001232\n",
      "total time for create_layer_from_safe_tensor: 0.040007829666137695\n",
      "total infer process time(including all above plus gpu compute): 23.6436\n",
      "total infer wall time(including all above plus gpu compute): 23.1887\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5365109452104662\n",
      "total time for compression_time: 9.993176221003523\n",
      "total time for create_layer_from_safe_tensor: 0.04304194450378418\n",
      "total infer process time(including all above plus gpu compute): 24.3412\n",
      "total infer wall time(including all above plus gpu compute): 24.0914\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5121480108082324\n",
      "total time for compression_time: 10.00318182699084\n",
      "total time for create_layer_from_safe_tensor: 0.04406118392944336\n",
      "total infer process time(including all above plus gpu compute): 24.5518\n",
      "total infer wall time(including all above plus gpu compute): 24.1047\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5173784224516567\n",
      "total time for compression_time: 10.041405823993045\n",
      "total time for create_layer_from_safe_tensor: 0.04253387451171875\n",
      "total infer process time(including all above plus gpu compute): 24.3104\n",
      "total infer wall time(including all above plus gpu compute): 23.9366\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5517894309596159\n",
      "total time for compression_time: 9.73261437000474\n",
      "total time for create_layer_from_safe_tensor: 0.04065513610839844\n",
      "total infer process time(including all above plus gpu compute): 23.8164\n",
      "total infer wall time(including all above plus gpu compute): 23.2045\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.812265320431834\n",
      "total time for compression_time: 9.63332994400298\n",
      "total time for create_layer_from_safe_tensor: 0.0436701774597168\n",
      "total infer process time(including all above plus gpu compute): 25.3268\n",
      "total infer wall time(including all above plus gpu compute): 24.0762\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:25<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 3.8455828083078814\n",
      "total time for compression_time: 9.759248171993022\n",
      "total time for create_layer_from_safe_tensor: 0.04341554641723633\n",
      "total infer process time(including all above plus gpu compute): 25.1019\n",
      "total infer wall time(including all above plus gpu compute): 26.0696\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6724768815965945\n",
      "total time for compression_time: 9.86138332500741\n",
      "total time for create_layer_from_safe_tensor: 0.044893503189086914\n",
      "total infer process time(including all above plus gpu compute): 25.8211\n",
      "total infer wall time(including all above plus gpu compute): 24.5653\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.7439207661136606\n",
      "total time for compression_time: 9.86949042201195\n",
      "total time for create_layer_from_safe_tensor: 0.04401803016662598\n",
      "total infer process time(including all above plus gpu compute): 24.1774\n",
      "total infer wall time(including all above plus gpu compute): 23.9703\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.726199942513631\n",
      "total time for compression_time: 9.76221887500651\n",
      "total time for create_layer_from_safe_tensor: 0.0455172061920166\n",
      "total infer process time(including all above plus gpu compute): 25.0522\n",
      "total infer wall time(including all above plus gpu compute): 23.9554\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5930059611073375\n",
      "total time for compression_time: 9.86131637699691\n",
      "total time for create_layer_from_safe_tensor: 0.048035383224487305\n",
      "total infer process time(including all above plus gpu compute): 24.0814\n",
      "total infer wall time(including all above plus gpu compute): 23.8073\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.58it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4699765429777472\n",
      "total time for compression_time: 9.864420773001257\n",
      "total time for create_layer_from_safe_tensor: 0.04409527778625488\n",
      "total infer process time(including all above plus gpu compute): 24.5486\n",
      "total infer wall time(including all above plus gpu compute): 24.0320\n",
      "<|begin_of_text|>What is the capital of France? A)Paris B)Rome C)Greece D)London. I think the answer is\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6782834551122505\n",
      "total time for compression_time: 9.847203490993707\n",
      "total time for create_layer_from_safe_tensor: 0.04329204559326172\n",
      "total infer process time(including all above plus gpu compute): 25.0564\n",
      "total infer wall time(including all above plus gpu compute): 23.7694\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5640740845774417\n",
      "total time for compression_time: 9.676824763002514\n",
      "total time for create_layer_from_safe_tensor: 0.04269909858703613\n",
      "total infer process time(including all above plus gpu compute): 24.6070\n",
      "total infer wall time(including all above plus gpu compute): 23.5407\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.7538077784429333\n",
      "total time for compression_time: 9.548483137999938\n",
      "total time for create_layer_from_safe_tensor: 0.042687416076660156\n",
      "total infer process time(including all above plus gpu compute): 24.6464\n",
      "total infer wall time(including all above plus gpu compute): 23.4856\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5501569909683894\n",
      "total time for compression_time: 9.828007061994867\n",
      "total time for create_layer_from_safe_tensor: 0.04366159439086914\n",
      "total infer process time(including all above plus gpu compute): 23.7882\n",
      "total infer wall time(including all above plus gpu compute): 23.4866\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4810935307104955\n",
      "total time for compression_time: 9.987380618993484\n",
      "total time for create_layer_from_safe_tensor: 0.0421142578125\n",
      "total infer process time(including all above plus gpu compute): 24.0512\n",
      "total infer wall time(including all above plus gpu compute): 23.7567\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.7586630729529134\n",
      "total time for compression_time: 9.91884742700131\n",
      "total time for create_layer_from_safe_tensor: 0.05037689208984375\n",
      "total infer process time(including all above plus gpu compute): 25.8219\n",
      "total infer wall time(including all above plus gpu compute): 24.6369\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6169644602559856\n",
      "total time for compression_time: 9.91669618999731\n",
      "total time for create_layer_from_safe_tensor: 0.04331207275390625\n",
      "total infer process time(including all above plus gpu compute): 24.3587\n",
      "total infer wall time(including all above plus gpu compute): 23.9830\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6356866125370289\n",
      "total time for compression_time: 9.668684506008503\n",
      "total time for create_layer_from_safe_tensor: 0.04258131980895996\n",
      "total infer process time(including all above plus gpu compute): 25.0880\n",
      "total infer wall time(including all above plus gpu compute): 23.8307\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.7148113634812034\n",
      "total time for compression_time: 9.85673399899133\n",
      "total time for create_layer_from_safe_tensor: 0.04246783256530762\n",
      "total infer process time(including all above plus gpu compute): 24.1654\n",
      "total infer wall time(including all above plus gpu compute): 23.8397\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5805903994751134\n",
      "total time for compression_time: 9.901922313003524\n",
      "total time for create_layer_from_safe_tensor: 0.043448686599731445\n",
      "total infer process time(including all above plus gpu compute): 25.2750\n",
      "total infer wall time(including all above plus gpu compute): 24.0797\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.715862751532768\n",
      "total time for compression_time: 9.55394220299604\n",
      "total time for create_layer_from_safe_tensor: 0.04367637634277344\n",
      "total infer process time(including all above plus gpu compute): 24.4702\n",
      "total infer wall time(including all above plus gpu compute): 23.9430\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5406876612014457\n",
      "total time for compression_time: 9.996614356010468\n",
      "total time for create_layer_from_safe_tensor: 0.04458427429199219\n",
      "total infer process time(including all above plus gpu compute): 24.1859\n",
      "total infer wall time(including all above plus gpu compute): 23.9539\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.592924067947024\n",
      "total time for compression_time: 9.809059669998533\n",
      "total time for create_layer_from_safe_tensor: 0.04352712631225586\n",
      "total infer process time(including all above plus gpu compute): 23.8911\n",
      "total infer wall time(including all above plus gpu compute): 23.4645\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5754270493671356\n",
      "total time for compression_time: 9.87366033199578\n",
      "total time for create_layer_from_safe_tensor: 0.046671390533447266\n",
      "total infer process time(including all above plus gpu compute): 24.5474\n",
      "total infer wall time(including all above plus gpu compute): 24.0356\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.7897267818243563\n",
      "total time for compression_time: 9.517224741002792\n",
      "total time for create_layer_from_safe_tensor: 0.043256521224975586\n",
      "total infer process time(including all above plus gpu compute): 24.5405\n",
      "total infer wall time(including all above plus gpu compute): 23.3567\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.7520490415536187\n",
      "total time for compression_time: 9.863025592998383\n",
      "total time for create_layer_from_safe_tensor: 0.04339766502380371\n",
      "total infer process time(including all above plus gpu compute): 24.3881\n",
      "total infer wall time(including all above plus gpu compute): 23.9763\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6826689573335898\n",
      "total time for compression_time: 9.869825806994413\n",
      "total time for create_layer_from_safe_tensor: 0.04467439651489258\n",
      "total infer process time(including all above plus gpu compute): 24.4440\n",
      "total infer wall time(including all above plus gpu compute): 24.2190\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4897051420248317\n",
      "total time for compression_time: 10.120463315006418\n",
      "total time for create_layer_from_safe_tensor: 0.043164968490600586\n",
      "total infer process time(including all above plus gpu compute): 24.2161\n",
      "total infer wall time(including all above plus gpu compute): 23.8793\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.467965309124338\n",
      "total time for compression_time: 9.924201066989554\n",
      "total time for create_layer_from_safe_tensor: 0.03996634483337402\n",
      "total infer process time(including all above plus gpu compute): 23.5657\n",
      "total infer wall time(including all above plus gpu compute): 23.2140\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.72it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.841799304691449\n",
      "total time for compression_time: 9.502203419002399\n",
      "total time for create_layer_from_safe_tensor: 0.038667917251586914\n",
      "total infer process time(including all above plus gpu compute): 24.4470\n",
      "total infer wall time(including all above plus gpu compute): 23.2105\n",
      "<|begin_of_text|>What is the capital of France? Why is it considered a very beautiful city? Who was Mona Lisa? Where does the name of the\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5883310956469359\n",
      "total time for compression_time: 9.8204089479932\n",
      "total time for create_layer_from_safe_tensor: 0.04547119140625\n",
      "total infer process time(including all above plus gpu compute): 24.1325\n",
      "total infer wall time(including all above plus gpu compute): 23.6415\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6596248039149941\n",
      "total time for compression_time: 9.872932921991378\n",
      "total time for create_layer_from_safe_tensor: 0.04464077949523926\n",
      "total infer process time(including all above plus gpu compute): 25.2716\n",
      "total infer wall time(including all above plus gpu compute): 24.1376\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5506559642144566\n",
      "total time for compression_time: 9.772811052005636\n",
      "total time for create_layer_from_safe_tensor: 0.04128384590148926\n",
      "total infer process time(including all above plus gpu compute): 23.5100\n",
      "total infer wall time(including all above plus gpu compute): 23.0401\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4195691131735657\n",
      "total time for compression_time: 9.898802898011127\n",
      "total time for create_layer_from_safe_tensor: 0.03684210777282715\n",
      "total infer process time(including all above plus gpu compute): 23.3706\n",
      "total infer wall time(including all above plus gpu compute): 22.9361\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.4380753055556852\n",
      "total time for compression_time: 9.720891950990335\n",
      "total time for create_layer_from_safe_tensor: 0.039342641830444336\n",
      "total infer process time(including all above plus gpu compute): 23.3646\n",
      "total infer wall time(including all above plus gpu compute): 23.0059\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5929716176215152\n",
      "total time for compression_time: 9.718058055001165\n",
      "total time for create_layer_from_safe_tensor: 0.039415597915649414\n",
      "total infer process time(including all above plus gpu compute): 24.3297\n",
      "total infer wall time(including all above plus gpu compute): 23.0654\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5840896991139743\n",
      "total time for compression_time: 9.783142861997476\n",
      "total time for create_layer_from_safe_tensor: 0.039702415466308594\n",
      "total infer process time(including all above plus gpu compute): 24.0944\n",
      "total infer wall time(including all above plus gpu compute): 23.3749\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.663572843764996\n",
      "total time for compression_time: 9.729713145996357\n",
      "total time for create_layer_from_safe_tensor: 0.03961753845214844\n",
      "total infer process time(including all above plus gpu compute): 24.1330\n",
      "total infer wall time(including all above plus gpu compute): 23.3997\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6688738725242729\n",
      "total time for compression_time: 9.793489608997334\n",
      "total time for create_layer_from_safe_tensor: 0.04000496864318848\n",
      "total infer process time(including all above plus gpu compute): 23.5240\n",
      "total infer wall time(including all above plus gpu compute): 23.3166\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5798138759855647\n",
      "total time for compression_time: 9.812617860006867\n",
      "total time for create_layer_from_safe_tensor: 0.040392160415649414\n",
      "total infer process time(including all above plus gpu compute): 24.7237\n",
      "total infer wall time(including all above plus gpu compute): 23.5229\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5597976384324284\n",
      "total time for compression_time: 9.580080872996405\n",
      "total time for create_layer_from_safe_tensor: 0.038580894470214844\n",
      "total infer process time(including all above plus gpu compute): 24.5143\n",
      "total infer wall time(including all above plus gpu compute): 23.3032\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:21<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.581969557386401\n",
      "total time for compression_time: 9.5502166170063\n",
      "total time for create_layer_from_safe_tensor: 0.03833627700805664\n",
      "total infer process time(including all above plus gpu compute): 24.0012\n",
      "total infer wall time(including all above plus gpu compute): 22.8271\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:24<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 3.5728044075385696\n",
      "total time for compression_time: 9.907150312004887\n",
      "total time for create_layer_from_safe_tensor: 0.06102776527404785\n",
      "total infer process time(including all above plus gpu compute): 24.5308\n",
      "total infer wall time(including all above plus gpu compute): 25.3632\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5600696111960133\n",
      "total time for compression_time: 9.806768844004182\n",
      "total time for create_layer_from_safe_tensor: 0.04071545600891113\n",
      "total infer process time(including all above plus gpu compute): 23.7511\n",
      "total infer wall time(including all above plus gpu compute): 23.4161\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5421776064795267\n",
      "total time for compression_time: 9.733167718990444\n",
      "total time for create_layer_from_safe_tensor: 0.03787803649902344\n",
      "total infer process time(including all above plus gpu compute): 24.1008\n",
      "total infer wall time(including all above plus gpu compute): 23.0191\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6805737698196026\n",
      "total time for compression_time: 9.853407315004006\n",
      "total time for create_layer_from_safe_tensor: 0.03911018371582031\n",
      "total infer process time(including all above plus gpu compute): 24.2970\n",
      "total infer wall time(including all above plus gpu compute): 23.5178\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.523530577775091\n",
      "total time for compression_time: 9.782021427992731\n",
      "total time for create_layer_from_safe_tensor: 0.03947114944458008\n",
      "total infer process time(including all above plus gpu compute): 23.4725\n",
      "total infer wall time(including all above plus gpu compute): 23.1631\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5000342225775967\n",
      "total time for compression_time: 9.834427227999186\n",
      "total time for create_layer_from_safe_tensor: 0.03987383842468262\n",
      "total infer process time(including all above plus gpu compute): 23.5459\n",
      "total infer wall time(including all above plus gpu compute): 23.2243\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5877984199814819\n",
      "total time for compression_time: 9.67954843899679\n",
      "total time for create_layer_from_safe_tensor: 0.04078340530395508\n",
      "total infer process time(including all above plus gpu compute): 23.6375\n",
      "total infer wall time(including all above plus gpu compute): 23.3535\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:21<00:00,  3.81it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.2999814165505086\n",
      "total time for compression_time: 9.67295139900125\n",
      "total time for create_layer_from_safe_tensor: 0.03769659996032715\n",
      "total infer process time(including all above plus gpu compute): 23.3402\n",
      "total infer wall time(including all above plus gpu compute): 22.6271\n",
      "<|begin_of_text|>What is the capital of France? - What Is The Capital of...\n",
      "https://whatisss.com/what-is-the-capital-of-fr\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5356968277537817\n",
      "total time for compression_time: 9.7695316440022\n",
      "total time for create_layer_from_safe_tensor: 0.04029035568237305\n",
      "total infer process time(including all above plus gpu compute): 23.4344\n",
      "total infer wall time(including all above plus gpu compute): 23.0711\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:23<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.3984132023961138\n",
      "total time for compression_time: 10.122829893001835\n",
      "total time for create_layer_from_safe_tensor: 0.04578685760498047\n",
      "total infer process time(including all above plus gpu compute): 24.3776\n",
      "total infer wall time(including all above plus gpu compute): 23.8619\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.1642040708902641\n",
      "total time for compression_time: 10.041067649995966\n",
      "total time for create_layer_from_safe_tensor: 0.04110288619995117\n",
      "total infer process time(including all above plus gpu compute): 23.8339\n",
      "total infer wall time(including all above plus gpu compute): 23.2853\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:24<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6010797771632497\n",
      "total time for compression_time: 10.226492806988972\n",
      "total time for create_layer_from_safe_tensor: 0.04699897766113281\n",
      "total infer process time(including all above plus gpu compute): 26.1867\n",
      "total infer wall time(including all above plus gpu compute): 25.7755\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.3733444361660077\n",
      "total time for compression_time: 9.921171411993782\n",
      "total time for create_layer_from_safe_tensor: 0.03983616828918457\n",
      "total infer process time(including all above plus gpu compute): 23.7677\n",
      "total infer wall time(including all above plus gpu compute): 23.3392\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.1923959974647005\n",
      "total time for compression_time: 10.095145392000632\n",
      "total time for create_layer_from_safe_tensor: 0.0426945686340332\n",
      "total infer process time(including all above plus gpu compute): 24.1506\n",
      "total infer wall time(including all above plus gpu compute): 23.5295\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.3145033207856613\n",
      "total time for compression_time: 10.022468439010481\n",
      "total time for create_layer_from_safe_tensor: 0.04091453552246094\n",
      "total infer process time(including all above plus gpu compute): 23.6682\n",
      "total infer wall time(including all above plus gpu compute): 23.1432\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.386897159547516\n",
      "total time for compression_time: 9.730846570997528\n",
      "total time for create_layer_from_safe_tensor: 0.040619850158691406\n",
      "total infer process time(including all above plus gpu compute): 23.5918\n",
      "total infer wall time(including all above plus gpu compute): 23.1505\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.6214156965252187\n",
      "total time for compression_time: 9.566688932991383\n",
      "total time for create_layer_from_safe_tensor: 0.042233943939208984\n",
      "total infer process time(including all above plus gpu compute): 24.5294\n",
      "total infer wall time(including all above plus gpu compute): 23.1705\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:22<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time for load_safe_tensor: 1.5885076576778374\n",
      "total time for compression_time: 9.68385028299599\n",
      "total time for create_layer_from_safe_tensor: 0.04186725616455078\n",
      "total infer process time(including all above plus gpu compute): 23.6072\n",
      "total infer wall time(including all above plus gpu compute): 23.4133\n",
      "new version of transfomer, no need to use BetterTransformer, try setting attn impl to sdpa...\n",
      "attn imp: <class 'transformers.models.llama.modeling_llama.LlamaSdpaAttention'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running layers(cuda:0):  99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 82/83 [00:22<00:00,  4.31it/s]"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "input_text = [\n",
    "        'What is the capital of France?'\n",
    "    ]\n",
    "\n",
    "input_tokens = model2.tokenizer(input_text,\n",
    "    return_tensors=\"pt\", \n",
    "    return_attention_mask=False, \n",
    "    truncation=True, \n",
    "    max_length=MAX_LENGTH, \n",
    "    padding=False)\n",
    "           \n",
    "generation_output = model2.generate(\n",
    "    input_tokens['input_ids'].cuda(), \n",
    "    max_new_tokens=20,\n",
    "    use_cache=True,\n",
    "    return_dict_in_generate=True)\n",
    "\n",
    "output = model2.tokenizer.decode(generation_output.sequences[0])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a1cb6-8a5f-46ef-aa3f-db4f65a331f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed9c2a-cd65-4b4e-b066-0449c72346a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f95866-418d-433e-86b4-657d1a9c5d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3345d453-b8d5-4197-ad1f-e697dda0bed0",
   "metadata": {},
   "source": [
    "# Model folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac78a463-68cc-4840-aab7-4b8ca35d9513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head.safetensors\t\t     model.layers.44.safetensors.done\n",
      "lm_head.safetensors.done\t     model.layers.45.safetensors\n",
      "model.embed_tokens.safetensors\t     model.layers.45.safetensors.done\n",
      "model.embed_tokens.safetensors.done  model.layers.46.safetensors\n",
      "model.layers.0.safetensors\t     model.layers.46.safetensors.done\n",
      "model.layers.0.safetensors.done      model.layers.47.safetensors\n",
      "model.layers.1.safetensors\t     model.layers.47.safetensors.done\n",
      "model.layers.1.safetensors.done      model.layers.48.safetensors\n",
      "model.layers.10.safetensors\t     model.layers.48.safetensors.done\n",
      "model.layers.10.safetensors.done     model.layers.49.safetensors\n",
      "model.layers.11.safetensors\t     model.layers.49.safetensors.done\n",
      "model.layers.11.safetensors.done     model.layers.5.safetensors\n",
      "model.layers.12.safetensors\t     model.layers.5.safetensors.done\n",
      "model.layers.12.safetensors.done     model.layers.50.safetensors\n",
      "model.layers.13.safetensors\t     model.layers.50.safetensors.done\n",
      "model.layers.13.safetensors.done     model.layers.51.safetensors\n",
      "model.layers.14.safetensors\t     model.layers.51.safetensors.done\n",
      "model.layers.14.safetensors.done     model.layers.52.safetensors\n",
      "model.layers.15.safetensors\t     model.layers.52.safetensors.done\n",
      "model.layers.15.safetensors.done     model.layers.53.safetensors\n",
      "model.layers.16.safetensors\t     model.layers.53.safetensors.done\n",
      "model.layers.16.safetensors.done     model.layers.54.safetensors\n",
      "model.layers.17.safetensors\t     model.layers.54.safetensors.done\n",
      "model.layers.17.safetensors.done     model.layers.55.safetensors\n",
      "model.layers.18.safetensors\t     model.layers.55.safetensors.done\n",
      "model.layers.18.safetensors.done     model.layers.56.safetensors\n",
      "model.layers.19.safetensors\t     model.layers.56.safetensors.done\n",
      "model.layers.19.safetensors.done     model.layers.57.safetensors\n",
      "model.layers.2.safetensors\t     model.layers.57.safetensors.done\n",
      "model.layers.2.safetensors.done      model.layers.58.safetensors\n",
      "model.layers.20.safetensors\t     model.layers.58.safetensors.done\n",
      "model.layers.20.safetensors.done     model.layers.59.safetensors\n",
      "model.layers.21.safetensors\t     model.layers.59.safetensors.done\n",
      "model.layers.21.safetensors.done     model.layers.6.safetensors\n",
      "model.layers.22.safetensors\t     model.layers.6.safetensors.done\n",
      "model.layers.22.safetensors.done     model.layers.60.safetensors\n",
      "model.layers.23.safetensors\t     model.layers.60.safetensors.done\n",
      "model.layers.23.safetensors.done     model.layers.61.safetensors\n",
      "model.layers.24.safetensors\t     model.layers.61.safetensors.done\n",
      "model.layers.24.safetensors.done     model.layers.62.safetensors\n",
      "model.layers.25.safetensors\t     model.layers.62.safetensors.done\n",
      "model.layers.25.safetensors.done     model.layers.63.safetensors\n",
      "model.layers.26.safetensors\t     model.layers.63.safetensors.done\n",
      "model.layers.26.safetensors.done     model.layers.64.safetensors\n",
      "model.layers.27.safetensors\t     model.layers.64.safetensors.done\n",
      "model.layers.27.safetensors.done     model.layers.65.safetensors\n",
      "model.layers.28.safetensors\t     model.layers.65.safetensors.done\n",
      "model.layers.28.safetensors.done     model.layers.66.safetensors\n",
      "model.layers.29.safetensors\t     model.layers.66.safetensors.done\n",
      "model.layers.29.safetensors.done     model.layers.67.safetensors\n",
      "model.layers.3.safetensors\t     model.layers.67.safetensors.done\n",
      "model.layers.3.safetensors.done      model.layers.68.safetensors\n",
      "model.layers.30.safetensors\t     model.layers.68.safetensors.done\n",
      "model.layers.30.safetensors.done     model.layers.69.safetensors\n",
      "model.layers.31.safetensors\t     model.layers.69.safetensors.done\n",
      "model.layers.31.safetensors.done     model.layers.7.safetensors\n",
      "model.layers.32.safetensors\t     model.layers.7.safetensors.done\n",
      "model.layers.32.safetensors.done     model.layers.70.safetensors\n",
      "model.layers.33.safetensors\t     model.layers.70.safetensors.done\n",
      "model.layers.33.safetensors.done     model.layers.71.safetensors\n",
      "model.layers.34.safetensors\t     model.layers.71.safetensors.done\n",
      "model.layers.34.safetensors.done     model.layers.72.safetensors\n",
      "model.layers.35.safetensors\t     model.layers.72.safetensors.done\n",
      "model.layers.35.safetensors.done     model.layers.73.safetensors\n",
      "model.layers.36.safetensors\t     model.layers.73.safetensors.done\n",
      "model.layers.36.safetensors.done     model.layers.74.safetensors\n",
      "model.layers.37.safetensors\t     model.layers.74.safetensors.done\n",
      "model.layers.37.safetensors.done     model.layers.75.safetensors\n",
      "model.layers.38.safetensors\t     model.layers.75.safetensors.done\n",
      "model.layers.38.safetensors.done     model.layers.76.safetensors\n",
      "model.layers.39.safetensors\t     model.layers.76.safetensors.done\n",
      "model.layers.39.safetensors.done     model.layers.77.safetensors\n",
      "model.layers.4.safetensors\t     model.layers.77.safetensors.done\n",
      "model.layers.4.safetensors.done      model.layers.78.safetensors\n",
      "model.layers.40.safetensors\t     model.layers.78.safetensors.done\n",
      "model.layers.40.safetensors.done     model.layers.79.safetensors\n",
      "model.layers.41.safetensors\t     model.layers.79.safetensors.done\n",
      "model.layers.41.safetensors.done     model.layers.8.safetensors\n",
      "model.layers.42.safetensors\t     model.layers.8.safetensors.done\n",
      "model.layers.42.safetensors.done     model.layers.9.safetensors\n",
      "model.layers.43.safetensors\t     model.layers.9.safetensors.done\n",
      "model.layers.43.safetensors.done     model.norm.safetensors\n",
      "model.layers.44.safetensors\t     model.norm.safetensors.done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/31389b50953688e4e542be53e6d2ab04d5c34e87/splitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac7d31-cbd0-4ea2-ae50-a7785548aa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
