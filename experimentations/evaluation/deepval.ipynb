{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b8328-7e57-4828-9b98-491845eca32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepeval transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75262e6-14d0-4c81-a5fb-5bb99c119db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepeval login --confident-api-key UeKlBLoFiPIL1zAlZjGvY7dEuXAT6xOH4nqdi4qRLmk="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d024261-8373-4883-9143-1b45eee7bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "class Mistral7B(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        model = self.load_model()\n",
    "\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\")#.to(device)\n",
    "        #model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Mistral 7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f377a-23fe-435f-86d8-ad4e65494c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e6de328-9caf-422f-83f6-304688ea06e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Write me a joke using the name of at least two vegetables\n",
      "\n",
      "What did the farmer say to the tomato in the morning?\n",
      "\n",
      "Ketchup!\n",
      "\n",
      "What did the cabbage say to the bean?\n",
      "\n",
      "You sound pea-sy\n",
      "\n",
      "Mum, I've got a very good joke for you...\n",
      "\n",
      "Two cabbages go into a bar...\n",
      "\n",
      "What's the difference between a pear and a banana?\n",
      "\n",
      "Apple-lantly nothing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mistral_7b = Mistral7B(model=model, tokenizer=tokenizer)\n",
    "print(mistral_7b.generate(\"Write me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4fe552-59f1-4326-a17b-bb6e76604525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
